
==> Audit <==
|------------|-------------------------------------|----------|----------|---------|---------------------|---------------------|
|  Command   |                Args                 | Profile  |   User   | Version |     Start Time      |      End Time       |
|------------|-------------------------------------|----------|----------|---------|---------------------|---------------------|
| start      |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 14:50 -03 |                     |
| start      |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 14:50 -03 |                     |
| start      |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 14:58 -03 |                     |
| start      |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 15:01 -03 |                     |
| start      |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 15:01 -03 | 08 Mar 24 15:03 -03 |
| start      |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 15:32 -03 | 08 Mar 24 15:33 -03 |
| start      |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 15:34 -03 | 08 Mar 24 15:34 -03 |
| stop       |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 16:02 -03 | 08 Mar 24 16:02 -03 |
| delete     |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 17:16 -03 | 08 Mar 24 17:16 -03 |
| start      |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 17:16 -03 | 08 Mar 24 17:17 -03 |
| stop       |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 17:25 -03 | 08 Mar 24 17:26 -03 |
| delete     |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 17:26 -03 | 08 Mar 24 17:26 -03 |
| start      |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 17:26 -03 | 08 Mar 24 17:26 -03 |
| service    | --namespace argocd                  | minikube | hamilton | v1.32.0 | 08 Mar 24 17:29 -03 | 08 Mar 24 17:32 -03 |
|            | argocd-server-nodeport --url        |          |          |         |                     |                     |
| start      | --memory=5120 --cpus=4              | minikube | hamilton | v1.32.0 | 08 Mar 24 17:32 -03 |                     |
|            | --vm-driver=virtualbox              |          |          |         |                     |                     |
| delete     |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 17:33 -03 | 08 Mar 24 17:33 -03 |
| start      | --memory=5120 --cpus=4              | minikube | hamilton | v1.32.0 | 08 Mar 24 17:33 -03 |                     |
|            | --vm-driver=virtualbox              |          |          |         |                     |                     |
| start      | --memory=5120 --cpus=4              | minikube | hamilton | v1.32.0 | 08 Mar 24 17:33 -03 |                     |
|            | --vm-driver=virtualbox              |          |          |         |                     |                     |
| service    |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 17:39 -03 |                     |
| dashboard  |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 17:39 -03 |                     |
| start      | --memory=5120 --cpus=4              | minikube | hamilton | v1.32.0 | 08 Mar 24 17:39 -03 |                     |
|            | --vm-driver=virtualbox              |          |          |         |                     |                     |
| start      | --memory=5120 --cpus=4              | minikube | hamilton | v1.32.0 | 08 Mar 24 17:40 -03 | 08 Mar 24 17:41 -03 |
|            | --vm-driver=docker                  |          |          |         |                     |                     |
| service    |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 17:47 -03 |                     |
| service    | default                             | minikube | hamilton | v1.32.0 | 08 Mar 24 17:48 -03 |                     |
| service    | argocd                              | minikube | hamilton | v1.32.0 | 08 Mar 24 17:48 -03 |                     |
| service    | kubernetes                          | minikube | hamilton | v1.32.0 | 08 Mar 24 17:49 -03 | 08 Mar 24 17:59 -03 |
| delete     | --all                               | minikube | hamilton | v1.32.0 | 08 Mar 24 18:00 -03 | 08 Mar 24 18:00 -03 |
| addons     | list                                | minikube | hamilton | v1.32.0 | 08 Mar 24 18:01 -03 | 08 Mar 24 18:01 -03 |
| kubectl    | -- get pod                          | minikube | hamilton | v1.32.0 | 08 Mar 24 18:03 -03 |                     |
| kubectl    | -- get pods                         | minikube | hamilton | v1.32.0 | 08 Mar 24 18:03 -03 |                     |
| kubectl    | -- get pods                         | minikube | hamilton | v1.32.0 | 08 Mar 24 18:03 -03 |                     |
| start      | --extra-config=apiserver.v=10       | minikube | hamilton | v1.32.0 | 08 Mar 24 18:06 -03 | 08 Mar 24 18:06 -03 |
|            | --extra-config=kubelet.max-pods=100 |          |          |         |                     |                     |
| service    | kubernetets                         | minikube | hamilton | v1.32.0 | 08 Mar 24 18:07 -03 |                     |
| service    |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 18:07 -03 |                     |
| service    |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 18:07 -03 |                     |
| service    | --all                               | minikube | hamilton | v1.32.0 | 08 Mar 24 18:07 -03 | 08 Mar 24 18:08 -03 |
| docker-env |                                     | minikube | hamilton | v1.32.0 | 08 Mar 24 18:08 -03 | 08 Mar 24 18:08 -03 |
| start      |                                     | minikube | hamilton | v1.32.0 | 14 Mar 24 18:48 -03 | 14 Mar 24 18:49 -03 |
| ip         |                                     | minikube | hamilton | v1.32.0 | 14 Mar 24 19:30 -03 | 14 Mar 24 19:30 -03 |
| ip         |                                     | minikube | hamilton | v1.32.0 | 14 Mar 24 19:39 -03 | 14 Mar 24 19:39 -03 |
| ip         |                                     | minikube | hamilton | v1.32.0 | 14 Mar 24 19:39 -03 | 14 Mar 24 19:39 -03 |
| ip         |                                     | minikube | hamilton | v1.32.0 | 14 Mar 24 19:39 -03 | 14 Mar 24 19:39 -03 |
| ip         |                                     | minikube | hamilton | v1.32.0 | 14 Mar 24 19:44 -03 | 14 Mar 24 19:44 -03 |
| start      |                                     | minikube | hamilton | v1.32.0 | 14 Mar 24 19:59 -03 | 14 Mar 24 20:00 -03 |
| image      | build .                             | minikube | hamilton | v1.32.0 | 14 Mar 24 20:36 -03 | 14 Mar 24 20:36 -03 |
| start      |                                     | minikube | hamilton | v1.32.0 | 05 Apr 24 18:20 -03 | 05 Apr 24 18:20 -03 |
| stop       |                                     | minikube | hamilton | v1.32.0 | 05 Apr 24 18:54 -03 | 05 Apr 24 18:54 -03 |
| start      |                                     | minikube | hamilton | v1.32.0 | 05 Apr 24 19:08 -03 | 05 Apr 24 19:08 -03 |
| image      | load hello-kube                     | minikube | hamilton | v1.32.0 | 05 Apr 24 19:10 -03 |                     |
| image      | load first-image                    | minikube | hamilton | v1.32.0 | 05 Apr 24 19:12 -03 |                     |
| image      | build -t second-image -f            | minikube | hamilton | v1.32.0 | 05 Apr 24 19:13 -03 | 05 Apr 24 19:13 -03 |
|            | ./Dockerfile .                      |          |          |         |                     |                     |
| service    | second-image --url                  | minikube | hamilton | v1.32.0 | 05 Apr 24 19:26 -03 |                     |
| service    | second-image --url                  | minikube | hamilton | v1.32.0 | 05 Apr 24 19:26 -03 |                     |
| image      | ls                                  | minikube | hamilton | v1.32.0 | 05 Apr 24 19:33 -03 | 05 Apr 24 19:33 -03 |
| image      | load hello-kube                     | minikube | hamilton | v1.32.0 | 05 Apr 24 19:34 -03 |                     |
| image      | ls                                  | minikube | hamilton | v1.32.0 | 05 Apr 24 19:34 -03 | 05 Apr 24 19:34 -03 |
| image      | load hello-kube                     | minikube | hamilton | v1.32.0 | 05 Apr 24 19:35 -03 |                     |
|------------|-------------------------------------|----------|----------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/04/05 19:08:21
Running on machine: Hamilton
Binary: Built with gc go1.21.3 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0405 19:08:21.943268   46934 out.go:296] Setting OutFile to fd 1 ...
I0405 19:08:21.943510   46934 out.go:309] Setting ErrFile to fd 2...
I0405 19:08:21.943716   46934 root.go:338] Updating PATH: /home/hamilton/.minikube/bin
I0405 19:08:21.944020   46934 out.go:303] Setting JSON to false
I0405 19:08:21.944999   46934 start.go:128] hostinfo: {"hostname":"Hamilton","uptime":2908,"bootTime":1712351993,"procs":54,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"20.04","kernelVersion":"5.15.146.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"2a1155df-5bd1-4fd3-8ccc-25286179ff2a"}
I0405 19:08:21.945067   46934 start.go:138] virtualization:  guest
I0405 19:08:21.949264   46934 out.go:177] 😄  minikube v1.32.0 on Ubuntu 20.04 (amd64)
I0405 19:08:21.953055   46934 notify.go:220] Checking for updates...
I0405 19:08:21.953349   46934 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0405 19:08:21.953450   46934 driver.go:378] Setting default libvirt URI to qemu:///system
I0405 19:08:22.010900   46934 docker.go:122] docker version: linux-25.0.4:Docker Engine - Community
I0405 19:08:22.010994   46934 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0405 19:08:22.102561   46934 info.go:266] docker info: {ID:72977195-7e84-4c1b-a02d-2ba214b19b11 Containers:21 ContainersRunning:1 ContainersPaused:0 ContainersStopped:20 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:38 OomKillDisable:true NGoroutines:60 SystemTime:2024-04-05 19:08:22.092858625 -0300 -03 LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 20.04.6 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8294309888 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Hamilton Labels:[] ExperimentalBuild:false ServerVersion:25.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.7]] Warnings:<nil>}}
I0405 19:08:22.102637   46934 docker.go:295] overlay module found
I0405 19:08:22.104944   46934 out.go:177] ✨  Using the docker driver based on existing profile
I0405 19:08:22.107198   46934 start.go:298] selected driver: docker
I0405 19:08:22.107207   46934 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:apiserver Key:v Value:10} {Component:kubelet Key:max-pods Value:100}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hamilton:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0405 19:08:22.107305   46934 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0405 19:08:22.107399   46934 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0405 19:08:22.183517   46934 info.go:266] docker info: {ID:72977195-7e84-4c1b-a02d-2ba214b19b11 Containers:21 ContainersRunning:1 ContainersPaused:0 ContainersStopped:20 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:38 OomKillDisable:true NGoroutines:60 SystemTime:2024-04-05 19:08:22.175123625 -0300 -03 LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 20.04.6 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8294309888 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Hamilton Labels:[] ExperimentalBuild:false ServerVersion:25.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.7]] Warnings:<nil>}}
I0405 19:08:22.184056   46934 cni.go:84] Creating CNI manager for ""
I0405 19:08:22.184066   46934 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0405 19:08:22.184071   46934 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:apiserver Key:v Value:10} {Component:kubelet Key:max-pods Value:100}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hamilton:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0405 19:08:22.187205   46934 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I0405 19:08:22.189826   46934 cache.go:121] Beginning downloading kic base image for docker with docker
I0405 19:08:22.192341   46934 out.go:177] 🚜  Pulling base image ...
I0405 19:08:22.195044   46934 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0405 19:08:22.195081   46934 preload.go:148] Found local preload: /home/hamilton/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0405 19:08:22.195087   46934 cache.go:56] Caching tarball of preloaded images
I0405 19:08:22.195149   46934 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0405 19:08:22.195211   46934 preload.go:174] Found /home/hamilton/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0405 19:08:22.195220   46934 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0405 19:08:22.195324   46934 profile.go:148] Saving config to /home/hamilton/.minikube/profiles/minikube/config.json ...
I0405 19:08:22.234695   46934 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0405 19:08:22.234710   46934 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0405 19:08:22.234758   46934 cache.go:194] Successfully downloaded all kic artifacts
I0405 19:08:22.234801   46934 start.go:365] acquiring machines lock for minikube: {Name:mkd417fb855e79645051be1864cd42f44399aae5 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0405 19:08:22.234907   46934 start.go:369] acquired machines lock for "minikube" in 77.348µs
I0405 19:08:22.234924   46934 start.go:96] Skipping create...Using existing machine configuration
I0405 19:08:22.234929   46934 fix.go:54] fixHost starting: 
I0405 19:08:22.235133   46934 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0405 19:08:22.269095   46934 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0405 19:08:22.269124   46934 fix.go:128] unexpected machine state, will restart: <nil>
I0405 19:08:22.271406   46934 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0405 19:08:22.275168   46934 cli_runner.go:164] Run: docker start minikube
I0405 19:08:22.695153   46934 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0405 19:08:22.746367   46934 kic.go:430] container "minikube" state is running.
I0405 19:08:22.746765   46934 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0405 19:08:22.788966   46934 profile.go:148] Saving config to /home/hamilton/.minikube/profiles/minikube/config.json ...
I0405 19:08:22.789165   46934 machine.go:88] provisioning docker machine ...
I0405 19:08:22.789180   46934 ubuntu.go:169] provisioning hostname "minikube"
I0405 19:08:22.789216   46934 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0405 19:08:22.837807   46934 main.go:141] libmachine: Using SSH client type: native
I0405 19:08:22.838100   46934 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32777 <nil> <nil>}
I0405 19:08:22.838108   46934 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0405 19:08:22.838799   46934 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:33574->127.0.0.1:32777: read: connection reset by peer
I0405 19:08:25.979610   46934 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0405 19:08:25.979692   46934 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0405 19:08:26.020136   46934 main.go:141] libmachine: Using SSH client type: native
I0405 19:08:26.020467   46934 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32777 <nil> <nil>}
I0405 19:08:26.020482   46934 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0405 19:08:26.138136   46934 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0405 19:08:26.138155   46934 ubuntu.go:175] set auth options {CertDir:/home/hamilton/.minikube CaCertPath:/home/hamilton/.minikube/certs/ca.pem CaPrivateKeyPath:/home/hamilton/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/hamilton/.minikube/machines/server.pem ServerKeyPath:/home/hamilton/.minikube/machines/server-key.pem ClientKeyPath:/home/hamilton/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/hamilton/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/hamilton/.minikube}
I0405 19:08:26.138170   46934 ubuntu.go:177] setting up certificates
I0405 19:08:26.138178   46934 provision.go:83] configureAuth start
I0405 19:08:26.138225   46934 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0405 19:08:26.190681   46934 provision.go:138] copyHostCerts
I0405 19:08:26.190723   46934 exec_runner.go:144] found /home/hamilton/.minikube/ca.pem, removing ...
I0405 19:08:26.190729   46934 exec_runner.go:203] rm: /home/hamilton/.minikube/ca.pem
I0405 19:08:26.190797   46934 exec_runner.go:151] cp: /home/hamilton/.minikube/certs/ca.pem --> /home/hamilton/.minikube/ca.pem (1082 bytes)
I0405 19:08:26.190963   46934 exec_runner.go:144] found /home/hamilton/.minikube/cert.pem, removing ...
I0405 19:08:26.190969   46934 exec_runner.go:203] rm: /home/hamilton/.minikube/cert.pem
I0405 19:08:26.191005   46934 exec_runner.go:151] cp: /home/hamilton/.minikube/certs/cert.pem --> /home/hamilton/.minikube/cert.pem (1127 bytes)
I0405 19:08:26.191056   46934 exec_runner.go:144] found /home/hamilton/.minikube/key.pem, removing ...
I0405 19:08:26.191060   46934 exec_runner.go:203] rm: /home/hamilton/.minikube/key.pem
I0405 19:08:26.191090   46934 exec_runner.go:151] cp: /home/hamilton/.minikube/certs/key.pem --> /home/hamilton/.minikube/key.pem (1679 bytes)
I0405 19:08:26.191134   46934 provision.go:112] generating server cert: /home/hamilton/.minikube/machines/server.pem ca-key=/home/hamilton/.minikube/certs/ca.pem private-key=/home/hamilton/.minikube/certs/ca-key.pem org=hamilton.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0405 19:08:26.549813   46934 provision.go:172] copyRemoteCerts
I0405 19:08:26.549864   46934 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0405 19:08:26.549897   46934 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0405 19:08:26.600983   46934 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32777 SSHKeyPath:/home/hamilton/.minikube/machines/minikube/id_rsa Username:docker}
I0405 19:08:26.692025   46934 ssh_runner.go:362] scp /home/hamilton/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1082 bytes)
I0405 19:08:26.710732   46934 ssh_runner.go:362] scp /home/hamilton/.minikube/machines/server.pem --> /etc/docker/server.pem (1208 bytes)
I0405 19:08:26.730920   46934 ssh_runner.go:362] scp /home/hamilton/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0405 19:08:26.750008   46934 provision.go:86] duration metric: configureAuth took 611.819856ms
I0405 19:08:26.750020   46934 ubuntu.go:193] setting minikube options for container-runtime
I0405 19:08:26.750150   46934 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0405 19:08:26.750186   46934 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0405 19:08:26.788924   46934 main.go:141] libmachine: Using SSH client type: native
I0405 19:08:26.789174   46934 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32777 <nil> <nil>}
I0405 19:08:26.789180   46934 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0405 19:08:26.908144   46934 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0405 19:08:26.908161   46934 ubuntu.go:71] root file system type: overlay
I0405 19:08:26.908292   46934 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0405 19:08:26.908345   46934 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0405 19:08:26.957787   46934 main.go:141] libmachine: Using SSH client type: native
I0405 19:08:26.958086   46934 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32777 <nil> <nil>}
I0405 19:08:26.958147   46934 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0405 19:08:27.090209   46934 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0405 19:08:27.090264   46934 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0405 19:08:27.136827   46934 main.go:141] libmachine: Using SSH client type: native
I0405 19:08:27.137131   46934 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32777 <nil> <nil>}
I0405 19:08:27.137145   46934 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0405 19:08:27.263509   46934 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0405 19:08:27.263528   46934 machine.go:91] provisioned docker machine in 4.474353523s
I0405 19:08:27.263543   46934 start.go:300] post-start starting for "minikube" (driver="docker")
I0405 19:08:27.263557   46934 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0405 19:08:27.263624   46934 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0405 19:08:27.263666   46934 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0405 19:08:27.312050   46934 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32777 SSHKeyPath:/home/hamilton/.minikube/machines/minikube/id_rsa Username:docker}
I0405 19:08:27.412451   46934 ssh_runner.go:195] Run: cat /etc/os-release
I0405 19:08:27.415988   46934 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0405 19:08:27.416014   46934 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0405 19:08:27.416023   46934 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0405 19:08:27.416029   46934 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0405 19:08:27.416037   46934 filesync.go:126] Scanning /home/hamilton/.minikube/addons for local assets ...
I0405 19:08:27.416080   46934 filesync.go:126] Scanning /home/hamilton/.minikube/files for local assets ...
I0405 19:08:27.416096   46934 start.go:303] post-start completed in 152.543951ms
I0405 19:08:27.416135   46934 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0405 19:08:27.416166   46934 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0405 19:08:27.467094   46934 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32777 SSHKeyPath:/home/hamilton/.minikube/machines/minikube/id_rsa Username:docker}
I0405 19:08:27.558818   46934 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0405 19:08:27.564066   46934 fix.go:56] fixHost completed within 5.329132139s
I0405 19:08:27.564081   46934 start.go:83] releasing machines lock for "minikube", held for 5.329165854s
I0405 19:08:27.564187   46934 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0405 19:08:27.615207   46934 ssh_runner.go:195] Run: cat /version.json
I0405 19:08:27.615255   46934 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0405 19:08:27.615349   46934 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0405 19:08:27.615411   46934 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0405 19:08:27.659890   46934 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32777 SSHKeyPath:/home/hamilton/.minikube/machines/minikube/id_rsa Username:docker}
I0405 19:08:27.661218   46934 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32777 SSHKeyPath:/home/hamilton/.minikube/machines/minikube/id_rsa Username:docker}
I0405 19:08:28.329510   46934 ssh_runner.go:195] Run: systemctl --version
I0405 19:08:28.333718   46934 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0405 19:08:28.337875   46934 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0405 19:08:28.354833   46934 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0405 19:08:28.354883   46934 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0405 19:08:28.362925   46934 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0405 19:08:28.362940   46934 start.go:472] detecting cgroup driver to use...
I0405 19:08:28.362964   46934 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0405 19:08:28.363051   46934 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0405 19:08:28.377929   46934 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0405 19:08:28.387125   46934 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0405 19:08:28.396253   46934 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0405 19:08:28.396291   46934 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0405 19:08:28.405261   46934 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0405 19:08:28.414746   46934 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0405 19:08:28.424451   46934 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0405 19:08:28.433937   46934 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0405 19:08:28.443334   46934 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0405 19:08:28.452785   46934 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0405 19:08:28.461819   46934 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0405 19:08:28.469108   46934 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0405 19:08:28.577324   46934 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0405 19:08:28.691772   46934 start.go:472] detecting cgroup driver to use...
I0405 19:08:28.691810   46934 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0405 19:08:28.691858   46934 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0405 19:08:28.704364   46934 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0405 19:08:28.704414   46934 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0405 19:08:28.717575   46934 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0405 19:08:28.733428   46934 ssh_runner.go:195] Run: which cri-dockerd
I0405 19:08:28.737026   46934 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0405 19:08:28.747057   46934 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0405 19:08:28.764143   46934 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0405 19:08:28.901629   46934 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0405 19:08:29.011089   46934 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0405 19:08:29.011208   46934 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0405 19:08:29.027443   46934 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0405 19:08:29.154026   46934 ssh_runner.go:195] Run: sudo systemctl restart docker
I0405 19:08:29.575716   46934 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0405 19:08:29.666965   46934 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0405 19:08:29.765900   46934 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0405 19:08:29.856318   46934 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0405 19:08:29.968995   46934 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0405 19:08:29.981465   46934 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0405 19:08:30.086065   46934 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0405 19:08:30.172866   46934 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0405 19:08:30.172934   46934 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0405 19:08:30.177446   46934 start.go:540] Will wait 60s for crictl version
I0405 19:08:30.177486   46934 ssh_runner.go:195] Run: which crictl
I0405 19:08:30.180508   46934 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0405 19:08:30.227525   46934 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0405 19:08:30.227564   46934 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0405 19:08:30.250052   46934 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0405 19:08:30.278641   46934 out.go:204] 🐳  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0405 19:08:30.278800   46934 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0405 19:08:30.319697   46934 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0405 19:08:30.322841   46934 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0405 19:08:30.332142   46934 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0405 19:08:30.375754   46934 out.go:177]     ▪ apiserver.v=10
I0405 19:08:30.379805   46934 out.go:177]     ▪ kubelet.max-pods=100
I0405 19:08:30.382319   46934 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0405 19:08:30.382377   46934 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0405 19:08:30.400748   46934 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5
jocatalin/kubernetes-bootcamp:v2
gcr.io/google-samples/kubernetes-bootcamp:v1

-- /stdout --
I0405 19:08:30.400770   46934 docker.go:601] Images already preloaded, skipping extraction
I0405 19:08:30.400812   46934 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0405 19:08:30.419383   46934 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5
jocatalin/kubernetes-bootcamp:v2
gcr.io/google-samples/kubernetes-bootcamp:v1

-- /stdout --
I0405 19:08:30.419394   46934 cache_images.go:84] Images are preloaded, skipping loading
I0405 19:08:30.419445   46934 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0405 19:08:30.483447   46934 cni.go:84] Creating CNI manager for ""
I0405 19:08:30.483459   46934 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0405 19:08:30.483477   46934 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0405 19:08:30.483493   46934 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota v:10] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0405 19:08:30.483632   46934 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
    v: "10"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0405 19:08:30.483747   46934 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --max-pods=100 --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:apiserver Key:v Value:10} {Component:kubelet Key:max-pods Value:100}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0405 19:08:30.483802   46934 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0405 19:08:30.491588   46934 binaries.go:44] Found k8s binaries, skipping transfer
I0405 19:08:30.491649   46934 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0405 19:08:30.498880   46934 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (384 bytes)
I0405 19:08:30.513636   46934 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0405 19:08:30.528602   46934 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2103 bytes)
I0405 19:08:30.543205   46934 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0405 19:08:30.546435   46934 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0405 19:08:30.555339   46934 certs.go:56] Setting up /home/hamilton/.minikube/profiles/minikube for IP: 192.168.49.2
I0405 19:08:30.555355   46934 certs.go:190] acquiring lock for shared ca certs: {Name:mk8c5da0abf93ef2c00fffc177079215e1a31e7b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0405 19:08:30.555470   46934 certs.go:199] skipping minikubeCA CA generation: /home/hamilton/.minikube/ca.key
I0405 19:08:30.555494   46934 certs.go:199] skipping proxyClientCA CA generation: /home/hamilton/.minikube/proxy-client-ca.key
I0405 19:08:30.555541   46934 certs.go:315] skipping minikube-user signed cert generation: /home/hamilton/.minikube/profiles/minikube/client.key
I0405 19:08:30.555570   46934 certs.go:315] skipping minikube signed cert generation: /home/hamilton/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0405 19:08:30.555593   46934 certs.go:315] skipping aggregator signed cert generation: /home/hamilton/.minikube/profiles/minikube/proxy-client.key
I0405 19:08:30.555860   46934 certs.go:437] found cert: /home/hamilton/.minikube/certs/home/hamilton/.minikube/certs/ca-key.pem (1679 bytes)
I0405 19:08:30.555882   46934 certs.go:437] found cert: /home/hamilton/.minikube/certs/home/hamilton/.minikube/certs/ca.pem (1082 bytes)
I0405 19:08:30.555899   46934 certs.go:437] found cert: /home/hamilton/.minikube/certs/home/hamilton/.minikube/certs/cert.pem (1127 bytes)
I0405 19:08:30.555914   46934 certs.go:437] found cert: /home/hamilton/.minikube/certs/home/hamilton/.minikube/certs/key.pem (1679 bytes)
I0405 19:08:30.556349   46934 ssh_runner.go:362] scp /home/hamilton/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0405 19:08:30.576796   46934 ssh_runner.go:362] scp /home/hamilton/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0405 19:08:30.596133   46934 ssh_runner.go:362] scp /home/hamilton/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0405 19:08:30.615267   46934 ssh_runner.go:362] scp /home/hamilton/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0405 19:08:30.634205   46934 ssh_runner.go:362] scp /home/hamilton/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0405 19:08:30.654617   46934 ssh_runner.go:362] scp /home/hamilton/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0405 19:08:30.674588   46934 ssh_runner.go:362] scp /home/hamilton/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0405 19:08:30.694430   46934 ssh_runner.go:362] scp /home/hamilton/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0405 19:08:30.713344   46934 ssh_runner.go:362] scp /home/hamilton/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0405 19:08:30.735372   46934 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0405 19:08:30.749865   46934 ssh_runner.go:195] Run: openssl version
I0405 19:08:30.754537   46934 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0405 19:08:30.762678   46934 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0405 19:08:30.765813   46934 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Mar  8 18:02 /usr/share/ca-certificates/minikubeCA.pem
I0405 19:08:30.765858   46934 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0405 19:08:30.771447   46934 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0405 19:08:30.778687   46934 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0405 19:08:30.781573   46934 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0405 19:08:30.786992   46934 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0405 19:08:30.792564   46934 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0405 19:08:30.798558   46934 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0405 19:08:30.804619   46934 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0405 19:08:30.809981   46934 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0405 19:08:30.815685   46934 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:apiserver Key:v Value:10} {Component:kubelet Key:max-pods Value:100}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/hamilton:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0405 19:08:30.815830   46934 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0405 19:08:30.833984   46934 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0405 19:08:30.841237   46934 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0405 19:08:30.841246   46934 kubeadm.go:636] restartCluster start
I0405 19:08:30.841280   46934 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0405 19:08:30.848917   46934 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0405 19:08:30.848976   46934 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0405 19:08:30.888112   46934 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in /home/hamilton/.kube/config
I0405 19:08:30.888175   46934 kubeconfig.go:146] "minikube" context is missing from /home/hamilton/.kube/config - will repair!
I0405 19:08:30.888501   46934 lock.go:35] WriteFile acquiring /home/hamilton/.kube/config: {Name:mk1d4e6438e418a1b772e044f8f3eb4320c932f9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0405 19:08:30.889571   46934 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0405 19:08:30.896881   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:30.896917   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:30.905167   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:30.905174   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:30.905205   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:30.913603   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:31.414367   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:31.414420   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:31.424731   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:31.914599   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:31.914654   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:31.925111   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:32.413930   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:32.413995   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:32.424306   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:32.913868   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:32.913922   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:32.924960   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:33.414625   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:33.414693   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:33.425563   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:33.914080   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:33.914135   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:33.924573   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:34.414153   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:34.414219   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:34.425094   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:34.914727   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:34.914789   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:34.925050   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:35.413742   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:35.413801   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:35.424909   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:35.914662   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:35.914744   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:35.925471   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:36.414029   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:36.414082   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:36.424591   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:36.914152   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:36.914205   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:36.925161   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:37.414026   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:37.414137   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:37.424657   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:37.914431   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:37.914504   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:37.925418   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:38.414009   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:38.414062   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:38.425226   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:38.913929   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:38.914045   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:38.925479   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:39.414112   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:39.414167   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:39.424846   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:39.914539   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:39.914595   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:39.925947   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:40.414599   46934 api_server.go:166] Checking apiserver status ...
I0405 19:08:40.414658   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0405 19:08:40.425508   46934 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0405 19:08:40.897458   46934 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0405 19:08:40.897473   46934 kubeadm.go:1128] stopping kube-system containers ...
I0405 19:08:40.897546   46934 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0405 19:08:40.921679   46934 docker.go:469] Stopping containers: [493e1983cc5a e1252aad335e e00d3a6b3948 79a731917c35 f233039c8f56 cc6185e003ea 6cf09f951ab5 fcc06fdf610b cb5d01d4b161 d3b3071b5179 355c99bdce53 5413aa353dce 979fa6c97325 5bf7a5d92ff4 66c3775b80ea 09b6df22aa0a 7ab4a703fc1b 39add3af1cbc ff9410dc9a39 99dc94ac122f b4ea0b2ec76c 31a8a33d8510 d334d346819b 9d78e62bc85d e6a7130a6af5 5ead7d5f4f01 267076de57a0]
I0405 19:08:40.921736   46934 ssh_runner.go:195] Run: docker stop 493e1983cc5a e1252aad335e e00d3a6b3948 79a731917c35 f233039c8f56 cc6185e003ea 6cf09f951ab5 fcc06fdf610b cb5d01d4b161 d3b3071b5179 355c99bdce53 5413aa353dce 979fa6c97325 5bf7a5d92ff4 66c3775b80ea 09b6df22aa0a 7ab4a703fc1b 39add3af1cbc ff9410dc9a39 99dc94ac122f b4ea0b2ec76c 31a8a33d8510 d334d346819b 9d78e62bc85d e6a7130a6af5 5ead7d5f4f01 267076de57a0
I0405 19:08:40.945102   46934 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0405 19:08:40.957795   46934 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0405 19:08:40.967035   46934 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Mar  8 21:06 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Apr  5 21:20 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Mar  8 21:06 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Apr  5 21:20 /etc/kubernetes/scheduler.conf

I0405 19:08:40.967085   46934 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0405 19:08:40.975885   46934 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0405 19:08:40.985130   46934 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0405 19:08:40.994114   46934 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0405 19:08:40.994182   46934 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0405 19:08:41.002927   46934 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0405 19:08:41.012285   46934 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0405 19:08:41.012327   46934 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0405 19:08:41.021180   46934 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0405 19:08:41.029754   46934 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0405 19:08:41.029771   46934 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0405 19:08:41.084910   46934 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0405 19:08:42.393515   46934 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.308576319s)
I0405 19:08:42.393550   46934 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0405 19:08:42.582938   46934 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0405 19:08:42.647625   46934 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0405 19:08:42.735226   46934 api_server.go:52] waiting for apiserver process to appear ...
I0405 19:08:42.735286   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0405 19:08:42.799261   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0405 19:08:43.316040   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0405 19:08:43.815767   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0405 19:08:44.315931   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0405 19:08:44.333227   46934 api_server.go:72] duration metric: took 1.598003959s to wait for apiserver process to appear ...
I0405 19:08:44.333240   46934 api_server.go:88] waiting for apiserver healthz status ...
I0405 19:08:44.333252   46934 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32774/healthz ...
I0405 19:08:44.333896   46934 api_server.go:269] stopped: https://127.0.0.1:32774/healthz: Get "https://127.0.0.1:32774/healthz": read tcp 127.0.0.1:45904->127.0.0.1:32774: read: connection reset by peer
I0405 19:08:44.333919   46934 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32774/healthz ...
I0405 19:08:44.334407   46934 api_server.go:269] stopped: https://127.0.0.1:32774/healthz: Get "https://127.0.0.1:32774/healthz": read tcp 127.0.0.1:45908->127.0.0.1:32774: read: connection reset by peer
I0405 19:08:44.835366   46934 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32774/healthz ...
I0405 19:08:47.703391   46934 api_server.go:279] https://127.0.0.1:32774/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0405 19:08:47.703414   46934 api_server.go:103] status: https://127.0.0.1:32774/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0405 19:08:47.703430   46934 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32774/healthz ...
I0405 19:08:47.801449   46934 api_server.go:279] https://127.0.0.1:32774/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[-]autoregister-completion failed: reason withheld
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0405 19:08:47.801478   46934 api_server.go:103] status: https://127.0.0.1:32774/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[-]autoregister-completion failed: reason withheld
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0405 19:08:47.834862   46934 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32774/healthz ...
I0405 19:08:47.905957   46934 api_server.go:279] https://127.0.0.1:32774/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0405 19:08:47.905978   46934 api_server.go:103] status: https://127.0.0.1:32774/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0405 19:08:48.335560   46934 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32774/healthz ...
I0405 19:08:48.343523   46934 api_server.go:279] https://127.0.0.1:32774/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0405 19:08:48.343548   46934 api_server.go:103] status: https://127.0.0.1:32774/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0405 19:08:48.834574   46934 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32774/healthz ...
I0405 19:08:48.903075   46934 api_server.go:279] https://127.0.0.1:32774/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0405 19:08:48.903102   46934 api_server.go:103] status: https://127.0.0.1:32774/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0405 19:08:49.334692   46934 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32774/healthz ...
I0405 19:08:49.396759   46934 api_server.go:279] https://127.0.0.1:32774/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0405 19:08:49.396790   46934 api_server.go:103] status: https://127.0.0.1:32774/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0405 19:08:49.835380   46934 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32774/healthz ...
I0405 19:08:49.901464   46934 api_server.go:279] https://127.0.0.1:32774/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0405 19:08:49.901491   46934 api_server.go:103] status: https://127.0.0.1:32774/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0405 19:08:50.335048   46934 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32774/healthz ...
I0405 19:08:50.401189   46934 api_server.go:279] https://127.0.0.1:32774/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0405 19:08:50.401214   46934 api_server.go:103] status: https://127.0.0.1:32774/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0405 19:08:50.834634   46934 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32774/healthz ...
I0405 19:08:50.844373   46934 api_server.go:279] https://127.0.0.1:32774/healthz returned 200:
ok
I0405 19:08:50.932276   46934 api_server.go:141] control plane version: v1.28.3
I0405 19:08:50.932291   46934 api_server.go:131] duration metric: took 6.599047097s to wait for apiserver health ...
I0405 19:08:50.932297   46934 cni.go:84] Creating CNI manager for ""
I0405 19:08:50.932307   46934 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0405 19:08:50.936527   46934 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0405 19:08:50.940153   46934 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0405 19:08:50.948331   46934 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0405 19:08:50.963319   46934 system_pods.go:43] waiting for kube-system pods to appear ...
I0405 19:08:50.971472   46934 system_pods.go:59] 7 kube-system pods found
I0405 19:08:50.971490   46934 system_pods.go:61] "coredns-5dd5756b68-tc7vg" [156d5966-bfbf-4866-be77-61bbc2dcbc16] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0405 19:08:50.971497   46934 system_pods.go:61] "etcd-minikube" [baef9070-9d1d-42b6-ac1f-fbfd89b0dd0a] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0405 19:08:50.971502   46934 system_pods.go:61] "kube-apiserver-minikube" [6f85ac03-6603-4ff7-a71f-30959d77987b] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0405 19:08:50.971507   46934 system_pods.go:61] "kube-controller-manager-minikube" [81f3e082-52ed-453f-b576-e8d4cf89bbb5] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0405 19:08:50.971512   46934 system_pods.go:61] "kube-proxy-k6xgr" [5226fbe1-ba11-454e-bd7e-3b7d7af21efc] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0405 19:08:50.971516   46934 system_pods.go:61] "kube-scheduler-minikube" [840cc3b9-47d0-498a-9438-d66f75430a5c] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0405 19:08:50.971519   46934 system_pods.go:61] "storage-provisioner" [69203bc5-1b0a-46b1-9662-747b7fe1e273] Running
I0405 19:08:50.971524   46934 system_pods.go:74] duration metric: took 8.196109ms to wait for pod list to return data ...
I0405 19:08:50.971528   46934 node_conditions.go:102] verifying NodePressure condition ...
I0405 19:08:50.975609   46934 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0405 19:08:50.975620   46934 node_conditions.go:123] node cpu capacity is 8
I0405 19:08:50.975628   46934 node_conditions.go:105] duration metric: took 4.096732ms to run NodePressure ...
I0405 19:08:50.975640   46934 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0405 19:08:51.172320   46934 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0405 19:08:51.205535   46934 ops.go:34] apiserver oom_adj: -16
I0405 19:08:51.205548   46934 kubeadm.go:640] restartCluster took 20.364296901s
I0405 19:08:51.205555   46934 kubeadm.go:406] StartCluster complete in 20.389878332s
I0405 19:08:51.205573   46934 settings.go:142] acquiring lock: {Name:mk02b87a899ce4484b94dc40754456973c34776c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0405 19:08:51.205655   46934 settings.go:150] Updating kubeconfig:  /home/hamilton/.kube/config
I0405 19:08:51.206288   46934 lock.go:35] WriteFile acquiring /home/hamilton/.kube/config: {Name:mk1d4e6438e418a1b772e044f8f3eb4320c932f9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0405 19:08:51.206653   46934 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0405 19:08:51.206878   46934 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0405 19:08:51.206877   46934 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0405 19:08:51.206945   46934 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0405 19:08:51.206960   46934 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0405 19:08:51.206966   46934 addons.go:240] addon storage-provisioner should already be in state true
I0405 19:08:51.206979   46934 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0405 19:08:51.206991   46934 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0405 19:08:51.207005   46934 host.go:66] Checking if "minikube" exists ...
I0405 19:08:51.207287   46934 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0405 19:08:51.207478   46934 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0405 19:08:51.214103   46934 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0405 19:08:51.214139   46934 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0405 19:08:51.223893   46934 out.go:177] 🔎  Verifying Kubernetes components...
I0405 19:08:51.232648   46934 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0405 19:08:51.268279   46934 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0405 19:08:51.266048   46934 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0405 19:08:51.270907   46934 addons.go:240] addon default-storageclass should already be in state true
I0405 19:08:51.270934   46934 host.go:66] Checking if "minikube" exists ...
I0405 19:08:51.270990   46934 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0405 19:08:51.271001   46934 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0405 19:08:51.271043   46934 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0405 19:08:51.271270   46934 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0405 19:08:51.354632   46934 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0405 19:08:51.354646   46934 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0405 19:08:51.354731   46934 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0405 19:08:51.357374   46934 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32777 SSHKeyPath:/home/hamilton/.minikube/machines/minikube/id_rsa Username:docker}
I0405 19:08:51.414799   46934 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0405 19:08:51.414885   46934 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0405 19:08:51.423093   46934 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32777 SSHKeyPath:/home/hamilton/.minikube/machines/minikube/id_rsa Username:docker}
I0405 19:08:51.463327   46934 api_server.go:52] waiting for apiserver process to appear ...
I0405 19:08:51.463727   46934 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0405 19:08:51.476156   46934 api_server.go:72] duration metric: took 261.986972ms to wait for apiserver process to appear ...
I0405 19:08:51.476168   46934 api_server.go:88] waiting for apiserver healthz status ...
I0405 19:08:51.476185   46934 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32774/healthz ...
I0405 19:08:51.482801   46934 api_server.go:279] https://127.0.0.1:32774/healthz returned 200:
ok
I0405 19:08:51.483014   46934 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0405 19:08:51.484620   46934 api_server.go:141] control plane version: v1.28.3
I0405 19:08:51.484635   46934 api_server.go:131] duration metric: took 8.462583ms to wait for apiserver health ...
I0405 19:08:51.484641   46934 system_pods.go:43] waiting for kube-system pods to appear ...
I0405 19:08:51.491898   46934 system_pods.go:59] 7 kube-system pods found
I0405 19:08:51.491914   46934 system_pods.go:61] "coredns-5dd5756b68-tc7vg" [156d5966-bfbf-4866-be77-61bbc2dcbc16] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0405 19:08:51.491920   46934 system_pods.go:61] "etcd-minikube" [baef9070-9d1d-42b6-ac1f-fbfd89b0dd0a] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0405 19:08:51.491927   46934 system_pods.go:61] "kube-apiserver-minikube" [6f85ac03-6603-4ff7-a71f-30959d77987b] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0405 19:08:51.491932   46934 system_pods.go:61] "kube-controller-manager-minikube" [81f3e082-52ed-453f-b576-e8d4cf89bbb5] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0405 19:08:51.491937   46934 system_pods.go:61] "kube-proxy-k6xgr" [5226fbe1-ba11-454e-bd7e-3b7d7af21efc] Running
I0405 19:08:51.491942   46934 system_pods.go:61] "kube-scheduler-minikube" [840cc3b9-47d0-498a-9438-d66f75430a5c] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0405 19:08:51.491946   46934 system_pods.go:61] "storage-provisioner" [69203bc5-1b0a-46b1-9662-747b7fe1e273] Running
I0405 19:08:51.491952   46934 system_pods.go:74] duration metric: took 7.307093ms to wait for pod list to return data ...
I0405 19:08:51.491959   46934 kubeadm.go:581] duration metric: took 277.797334ms to wait for : map[apiserver:true system_pods:true] ...
I0405 19:08:51.491969   46934 node_conditions.go:102] verifying NodePressure condition ...
I0405 19:08:51.496679   46934 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0405 19:08:51.496691   46934 node_conditions.go:123] node cpu capacity is 8
I0405 19:08:51.496700   46934 node_conditions.go:105] duration metric: took 4.727803ms to run NodePressure ...
I0405 19:08:51.496712   46934 start.go:228] waiting for startup goroutines ...
I0405 19:08:51.524322   46934 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0405 19:08:52.282807   46934 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0405 19:08:52.288710   46934 addons.go:502] enable addons completed in 1.081835083s: enabled=[storage-provisioner default-storageclass]
I0405 19:08:52.288765   46934 start.go:233] waiting for cluster config update ...
I0405 19:08:52.288774   46934 start.go:242] writing updated cluster config ...
I0405 19:08:52.288976   46934 ssh_runner.go:195] Run: rm -f paused
I0405 19:08:52.392628   46934 start.go:600] kubectl: 1.29.2, cluster: 1.28.3 (minor skew: 1)
I0405 19:08:52.396083   46934 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Apr 05 22:08:43 minikube cri-dockerd[1217]: time="2024-04-05T22:08:43Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"9d78e62bc85d98ff02595f10a08b0a0c03ff83a649908087709f022bde8ec02d\". Proceed without further sandbox information."
Apr 05 22:08:43 minikube cri-dockerd[1217]: time="2024-04-05T22:08:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/23eb67c245aae342f3834ce26bf31cd4fac49b5be96dc8e542425d6b78c1b250/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Apr 05 22:08:43 minikube cri-dockerd[1217]: time="2024-04-05T22:08:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/552015790dbe2076c5f96163503e400f4496cb2007006c6b8c2b5d8ab1213203/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Apr 05 22:08:43 minikube cri-dockerd[1217]: time="2024-04-05T22:08:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e5578a64b9079ceaa8d58c68842336bb8f0b2892687e37133aa200f5666ea6ab/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Apr 05 22:08:43 minikube cri-dockerd[1217]: time="2024-04-05T22:08:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ee1894fe53c61939dcfd1d2143d77c26e4099b1cee0dd7f55b6d0e74f17c5c6c/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Apr 05 22:08:44 minikube cri-dockerd[1217]: time="2024-04-05T22:08:44Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-tc7vg_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"f233039c8f56620a48de4bc1e454e6268151afc49b549d1f0ebf88972f2b1880\""
Apr 05 22:08:44 minikube cri-dockerd[1217]: time="2024-04-05T22:08:44Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-bootcamp-65df967b7f-zs9j5_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"5b66bb9560194002523a1feab5ea06693b56ab9fbfea1ccabb6644953c8794ec\""
Apr 05 22:08:44 minikube cri-dockerd[1217]: time="2024-04-05T22:08:44Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-bootcamp-65df967b7f-p68vd_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"a53848c94d67527c02bc19f5edcda5d9f2b95687bc2146299eaacf084f563dc0\""
Apr 05 22:08:48 minikube cri-dockerd[1217]: time="2024-04-05T22:08:48Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Apr 05 22:08:48 minikube cri-dockerd[1217]: time="2024-04-05T22:08:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/400cb110a5c53be1deed9dc5c9983d234b8cfbb484dd59d182347550ac8e1a74/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Apr 05 22:08:49 minikube cri-dockerd[1217]: time="2024-04-05T22:08:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a1fc3a43cf70eebd805980665d0d59f9a5eb982ac87651d6d37c83deea284dfc/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Apr 05 22:08:49 minikube cri-dockerd[1217]: time="2024-04-05T22:08:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ef2aec32c91957df5cb040889028530f3a8b2008fbc979955e3a6626dfd27a08/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 05 22:08:49 minikube cri-dockerd[1217]: time="2024-04-05T22:08:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/827de9b981d00e9d55c525e43715b11d213eaca717f0b5a10c8f66075881387a/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Apr 05 22:08:49 minikube cri-dockerd[1217]: time="2024-04-05T22:08:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1286cb3494a2ff7b24896029ad38109a45e670bd6a007ef1f1607edd2a007e79/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 05 22:09:05 minikube cri-dockerd[1217]: time="2024-04-05T22:09:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5d2f1528280123b832deb7086b2752aaf9954306a649fc0003c351b5c08e517c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 05 22:09:19 minikube dockerd[972]: time="2024-04-05T22:09:19.815143073Z" level=info msg="ignoring event" container=001f0e213c744c3306e30c2b3fd1a9bb7ace8d404473da067594c1b6dc273553 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 05 22:13:39 minikube dockerd[972]: time="2024-04-05T22:13:39.903199224Z" level=warning msg="no trace recorder found, skipping"
Apr 05 22:14:22 minikube cri-dockerd[1217]: time="2024-04-05T22:14:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6c2fce4298ce8e3db59bec3321248401e9f145863124c5a41a8bd3dac3a3cdab/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 05 22:22:10 minikube cri-dockerd[1217]: time="2024-04-05T22:22:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b39b932c7be5359c44d096923b31f49563ca0a8050f30a77488257bbecdabd28/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 05 22:22:13 minikube dockerd[972]: time="2024-04-05T22:22:13.523016416Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 05 22:22:13 minikube dockerd[972]: time="2024-04-05T22:22:13.523099835Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 05 22:22:26 minikube dockerd[972]: time="2024-04-05T22:22:26.755016634Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 05 22:22:26 minikube dockerd[972]: time="2024-04-05T22:22:26.755090565Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 05 22:22:53 minikube dockerd[972]: time="2024-04-05T22:22:53.924125583Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 05 22:22:53 minikube dockerd[972]: time="2024-04-05T22:22:53.924187501Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 05 22:23:47 minikube dockerd[972]: time="2024-04-05T22:23:47.962078344Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 05 22:23:47 minikube dockerd[972]: time="2024-04-05T22:23:47.962159830Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 05 22:25:24 minikube dockerd[972]: time="2024-04-05T22:25:24.649602458Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 05 22:25:24 minikube dockerd[972]: time="2024-04-05T22:25:24.649685767Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 05 22:28:06 minikube dockerd[972]: time="2024-04-05T22:28:06.845870173Z" level=info msg="ignoring event" container=b39b932c7be5359c44d096923b31f49563ca0a8050f30a77488257bbecdabd28 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 05 22:28:07 minikube cri-dockerd[1217]: time="2024-04-05T22:28:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/aa43e65f5abb00c7ffa13251a22f7e3e4557fcd13acfaf6997e145570ac03893/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 05 22:28:09 minikube dockerd[972]: time="2024-04-05T22:28:09.942535556Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 05 22:28:09 minikube dockerd[972]: time="2024-04-05T22:28:09.942663260Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 05 22:28:27 minikube dockerd[972]: time="2024-04-05T22:28:27.396461551Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 05 22:28:27 minikube dockerd[972]: time="2024-04-05T22:28:27.396546563Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 05 22:28:57 minikube dockerd[972]: time="2024-04-05T22:28:57.321999321Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 05 22:28:57 minikube dockerd[972]: time="2024-04-05T22:28:57.322087098Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 05 22:29:42 minikube dockerd[972]: time="2024-04-05T22:29:42.959726947Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 05 22:29:42 minikube dockerd[972]: time="2024-04-05T22:29:42.959778205Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 05 22:30:29 minikube dockerd[972]: time="2024-04-05T22:30:29.745117986Z" level=info msg="ignoring event" container=569b9aca5ecd857c763633bdc4b6e9056facb82ed11d41ebe58b130ae49275c9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 05 22:30:29 minikube cri-dockerd[1217]: time="2024-04-05T22:30:29Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"second-image_default\": unexpected command output nsenter: cannot open /proc/9495/ns/net: No such file or directory\n with error: exit status 1"
Apr 05 22:30:29 minikube dockerd[972]: time="2024-04-05T22:30:29.957545651Z" level=info msg="ignoring event" container=6c2fce4298ce8e3db59bec3321248401e9f145863124c5a41a8bd3dac3a3cdab module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 05 22:30:36 minikube dockerd[972]: time="2024-04-05T22:30:36.852464115Z" level=info msg="ignoring event" container=aa43e65f5abb00c7ffa13251a22f7e3e4557fcd13acfaf6997e145570ac03893 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 05 22:30:37 minikube cri-dockerd[1217]: time="2024-04-05T22:30:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/083c9196ce7803e650818e6c31f68b90c643f2f2a0b09d48c7f9f12af5ac12b4/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 05 22:30:39 minikube dockerd[972]: time="2024-04-05T22:30:39.654624009Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 05 22:30:39 minikube dockerd[972]: time="2024-04-05T22:30:39.654657663Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 05 22:30:56 minikube dockerd[972]: time="2024-04-05T22:30:56.375974119Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 05 22:30:56 minikube dockerd[972]: time="2024-04-05T22:30:56.376065593Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 05 22:31:01 minikube dockerd[972]: time="2024-04-05T22:31:01.714395213Z" level=info msg="ignoring event" container=083c9196ce7803e650818e6c31f68b90c643f2f2a0b09d48c7f9f12af5ac12b4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 05 22:31:47 minikube cri-dockerd[1217]: time="2024-04-05T22:31:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/344cb375db8573c207af2b32faf52dcf067648ea31d68fdcf89848b8c9b30d2b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 05 22:31:49 minikube dockerd[972]: time="2024-04-05T22:31:49.821387611Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 05 22:31:49 minikube dockerd[972]: time="2024-04-05T22:31:49.821449689Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 05 22:32:03 minikube dockerd[972]: time="2024-04-05T22:32:03.020526381Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 05 22:32:03 minikube dockerd[972]: time="2024-04-05T22:32:03.020590703Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 05 22:32:29 minikube dockerd[972]: time="2024-04-05T22:32:29.906182624Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 05 22:32:29 minikube dockerd[972]: time="2024-04-05T22:32:29.906254531Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 05 22:33:21 minikube dockerd[972]: time="2024-04-05T22:33:21.129965273Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 05 22:33:21 minikube dockerd[972]: time="2024-04-05T22:33:21.130040396Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 05 22:34:51 minikube dockerd[972]: time="2024-04-05T22:34:51.039761441Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 05 22:34:51 minikube dockerd[972]: time="2024-04-05T22:34:51.039838688Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
e00eca37dec8a       6e38f40d628db       26 minutes ago      Running             storage-provisioner       9                   400cb110a5c53       storage-provisioner
f5a0724ed94eb       b6556396ebd45       27 minutes ago      Running             kubernetes-bootcamp       3                   1286cb3494a2f       kubernetes-bootcamp-65df967b7f-zs9j5
5e96aae89347d       ead0a4a53df89       27 minutes ago      Running             coredns                   4                   827de9b981d00       coredns-5dd5756b68-tc7vg
9b01c8f3b0b32       b6556396ebd45       27 minutes ago      Running             kubernetes-bootcamp       3                   ef2aec32c9195       kubernetes-bootcamp-65df967b7f-p68vd
022ee63468b01       bfc896cf80fba       27 minutes ago      Running             kube-proxy                4                   a1fc3a43cf70e       kube-proxy-k6xgr
001f0e213c744       6e38f40d628db       27 minutes ago      Exited              storage-provisioner       8                   400cb110a5c53       storage-provisioner
1b9f043df7f94       6d1b4fd1b182d       27 minutes ago      Running             kube-scheduler            4                   ee1894fe53c61       kube-scheduler-minikube
9fdad6b3b0e88       10baa1ca17068       27 minutes ago      Running             kube-controller-manager   4                   e5578a64b9079       kube-controller-manager-minikube
38557dfd27ea1       5374347291230       27 minutes ago      Running             kube-apiserver            4                   552015790dbe2       kube-apiserver-minikube
6c3fcf67d556e       73deb9a3f7025       27 minutes ago      Running             etcd                      4                   23eb67c245aae       etcd-minikube
722e6449351b2       b6556396ebd45       About an hour ago   Exited              kubernetes-bootcamp       2                   5b66bb9560194       kubernetes-bootcamp-65df967b7f-zs9j5
e1252aad335e0       ead0a4a53df89       About an hour ago   Exited              coredns                   3                   f233039c8f566       coredns-5dd5756b68-tc7vg
b87b3119bba22       b6556396ebd45       About an hour ago   Exited              kubernetes-bootcamp       2                   a53848c94d675       kubernetes-bootcamp-65df967b7f-p68vd
e00d3a6b39489       bfc896cf80fba       About an hour ago   Exited              kube-proxy                3                   cc6185e003ea9       kube-proxy-k6xgr
fcc06fdf610b7       5374347291230       About an hour ago   Exited              kube-apiserver            3                   5413aa353dce6       kube-apiserver-minikube
cb5d01d4b161c       10baa1ca17068       About an hour ago   Exited              kube-controller-manager   3                   979fa6c973257       kube-controller-manager-minikube
d3b3071b5179b       6d1b4fd1b182d       About an hour ago   Exited              kube-scheduler            3                   5bf7a5d92ff4d       kube-scheduler-minikube
355c99bdce53a       73deb9a3f7025       About an hour ago   Exited              etcd                      3                   66c3775b80ea3       etcd-minikube


==> coredns [5e96aae89347] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:59211 - 46171 "HINFO IN 1043989653536116397.496994926027500005. udp 56 false 512" NXDOMAIN qr,rd,ra 131 0.152685797s


==> coredns [e1252aad335e] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:60690 - 8494 "HINFO IN 525957748576604403.1229661246390193206. udp 56 false 512" NXDOMAIN qr,rd,ra 131 0.357659867s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_03_08T18_06_42_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 08 Mar 2024 21:06:37 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 05 Apr 2024 22:36:12 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 05 Apr 2024 22:34:09 +0000   Fri, 08 Mar 2024 21:06:35 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 05 Apr 2024 22:34:09 +0000   Fri, 08 Mar 2024 21:06:35 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 05 Apr 2024 22:34:09 +0000   Fri, 08 Mar 2024 21:06:35 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 05 Apr 2024 22:34:09 +0000   Fri, 08 Mar 2024 21:06:42 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8099912Ki
  pods:               100
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8099912Ki
  pods:               100
System Info:
  Machine ID:                 0c464aa443c840c2afd0c7449325992a
  System UUID:                0c464aa443c840c2afd0c7449325992a
  Boot ID:                    9b98709a-b5cd-4327-bbe4-a95ca121521a
  Kernel Version:             5.15.146.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---
  default                     hello-kube                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         27m
  default                     hello-kube-59d66544fc-76gv5             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4m25s
  default                     kubernetes-bootcamp-65df967b7f-p68vd    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21d
  default                     kubernetes-bootcamp-65df967b7f-zs9j5    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21d
  kube-system                 coredns-5dd5756b68-tc7vg                100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     28d
  kube-system                 etcd-minikube                           100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         28d
  kube-system                 kube-apiserver-minikube                 250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         28d
  kube-system                 kube-controller-manager-minikube        200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         28d
  kube-system                 kube-proxy-k6xgr                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         28d
  kube-system                 kube-scheduler-minikube                 100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         28d
  kube-system                 storage-provisioner                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         28d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 75m                kube-proxy       
  Normal  Starting                 27m                kube-proxy       
  Normal  Starting                 75m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  75m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasNoDiskPressure    75m (x8 over 75m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     75m (x7 over 75m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeHasSufficientMemory  75m (x8 over 75m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  RegisteredNode           75m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 27m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  27m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  27m (x8 over 27m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    27m (x8 over 27m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     27m (x7 over 27m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           27m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Apr 5 21:19] PCI: Fatal: No config space access function found
[  +0.017098] PCI: System does not support PCI
[  +0.018180] kvm: no hardware support
[  +1.547125] FS-Cache: Duplicate cookie detected
[  +0.000586] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.000589] FS-Cache: O-cookie d=000000003a97852c{9P.session} n=0000000099ca777d
[  +0.000806] FS-Cache: O-key=[10] '34323934393337343535'
[  +0.000445] FS-Cache: N-cookie c=00000005 [p=00000002 fl=2 nc=0 na=1]
[  +0.000493] FS-Cache: N-cookie d=000000003a97852c{9P.session} n=00000000e83fa2b3
[  +0.000604] FS-Cache: N-key=[10] '34323934393337343535'
[  +0.004945] FS-Cache: Duplicate cookie detected
[  +0.000509] FS-Cache: O-cookie c=00000006 [p=00000002 fl=222 nc=0 na=1]
[  +0.000621] FS-Cache: O-cookie d=000000003a97852c{9P.session} n=000000003542545d
[  +0.000741] FS-Cache: O-key=[10] '34323934393337343536'
[  +0.000508] FS-Cache: N-cookie c=00000007 [p=00000002 fl=2 nc=0 na=1]
[  +0.000642] FS-Cache: N-cookie d=000000003a97852c{9P.session} n=00000000616dc940
[  +0.000813] FS-Cache: N-key=[10] '34323934393337343536'
[  +0.674487] /sbin/ldconfig: 
[  +0.000004] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.212882] FS-Cache: Duplicate cookie detected
[  +0.000543] FS-Cache: O-cookie c=0000000b [p=00000002 fl=222 nc=0 na=1]
[  +0.000521] FS-Cache: O-cookie d=000000003a97852c{9P.session} n=0000000034888cf8
[  +0.000742] FS-Cache: O-key=[10] '34323934393337353435'
[  +0.000438] FS-Cache: N-cookie c=0000000c [p=00000002 fl=2 nc=0 na=1]
[  +0.000496] FS-Cache: N-cookie d=000000003a97852c{9P.session} n=00000000021c6e0b
[  +0.000601] FS-Cache: N-key=[10] '34323934393337353435'
[  +0.138576] /sbin/ldconfig.real: 
[  +0.000004] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.368098] FS-Cache: Duplicate cookie detected
[  +0.000590] FS-Cache: O-cookie c=0000000f [p=00000002 fl=222 nc=0 na=1]
[  +0.000764] FS-Cache: O-cookie d=000000003a97852c{9P.session} n=0000000068796eff
[  +0.001014] FS-Cache: O-key=[10] '34323934393337353936'
[  +0.000653] FS-Cache: N-cookie c=00000010 [p=00000002 fl=2 nc=0 na=1]
[  +0.000615] FS-Cache: N-cookie d=000000003a97852c{9P.session} n=0000000022f47c4b
[  +0.000623] FS-Cache: N-key=[10] '34323934393337353936'
[  +0.247713] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000847] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000766] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000955] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.276893] Failed to connect to bus: No such file or directory
[  +0.279903] systemd-journald[61]: File /var/log/journal/2a1155df5bd14fd38ccc25286179ff2a/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Apr 5 21:20] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +2.211882] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001326] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001501] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001965] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001439] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001509] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001488] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002658] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001471] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001300] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001928] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001524] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001122] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001341] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.442610] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[Apr 5 22:29] hrtimer: interrupt took 729831 ns


==> etcd [355c99bdce53] <==
{"level":"warn","ts":"2024-04-05T21:22:50.526307Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-05T21:22:50.201503Z","time spent":"324.774905ms","remote":"127.0.0.1:57546","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-04-05T21:22:50.526254Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"114.564123ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-04-05T21:22:50.526437Z","caller":"traceutil/trace.go:171","msg":"trace[1203910031] range","detail":"{range_begin:/registry/secrets/; range_end:/registry/secrets0; response_count:0; response_revision:22700; }","duration":"114.742508ms","start":"2024-04-05T21:22:50.411673Z","end":"2024-04-05T21:22:50.526416Z","steps":["trace[1203910031] 'agreement among raft nodes before linearized reading'  (duration: 114.540729ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-05T21:22:50.526675Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-05T21:22:50.032692Z","time spent":"493.558081ms","remote":"127.0.0.1:57694","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:22697 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-04-05T21:22:57.603946Z","caller":"traceutil/trace.go:171","msg":"trace[1723656052] linearizableReadLoop","detail":"{readStateIndex:28318; appliedIndex:28317; }","duration":"491.317218ms","start":"2024-04-05T21:22:57.112603Z","end":"2024-04-05T21:22:57.603921Z","steps":["trace[1723656052] 'read index received'  (duration: 491.10287ms)","trace[1723656052] 'applied index is now lower than readState.Index'  (duration: 213.366µs)"],"step_count":2}
{"level":"warn","ts":"2024-04-05T21:22:57.604132Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"300.400174ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/\" range_end:\"/registry/apiextensions.k8s.io/customresourcedefinitions0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2024-04-05T21:22:57.604154Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"491.601215ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-04-05T21:22:57.604189Z","caller":"traceutil/trace.go:171","msg":"trace[1612964202] range","detail":"{range_begin:/registry/flowschemas/; range_end:/registry/flowschemas0; response_count:0; response_revision:22704; }","duration":"491.641341ms","start":"2024-04-05T21:22:57.112538Z","end":"2024-04-05T21:22:57.604179Z","steps":["trace[1612964202] 'agreement among raft nodes before linearized reading'  (duration: 491.568372ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-05T21:22:57.604252Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.822704ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-04-05T21:22:57.604294Z","caller":"traceutil/trace.go:171","msg":"trace[1661491597] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:22704; }","duration":"107.861398ms","start":"2024-04-05T21:22:57.49642Z","end":"2024-04-05T21:22:57.604282Z","steps":["trace[1661491597] 'agreement among raft nodes before linearized reading'  (duration: 107.804018ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-05T21:22:57.604291Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-05T21:22:57.112521Z","time spent":"491.759016ms","remote":"127.0.0.1:57926","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":13,"response size":32,"request content":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true "}
{"level":"info","ts":"2024-04-05T21:22:57.604215Z","caller":"traceutil/trace.go:171","msg":"trace[51482227] transaction","detail":"{read_only:false; response_revision:22704; number_of_response:1; }","duration":"964.859582ms","start":"2024-04-05T21:22:56.639342Z","end":"2024-04-05T21:22:57.604201Z","steps":["trace[51482227] 'process raft request'  (duration: 964.429983ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-05T21:22:57.604518Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-05T21:22:56.639328Z","time spent":"965.123744ms","remote":"127.0.0.1:57694","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:22703 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-04-05T21:22:57.604209Z","caller":"traceutil/trace.go:171","msg":"trace[1807872569] range","detail":"{range_begin:/registry/apiextensions.k8s.io/customresourcedefinitions/; range_end:/registry/apiextensions.k8s.io/customresourcedefinitions0; response_count:0; response_revision:22704; }","duration":"300.460709ms","start":"2024-04-05T21:22:57.303693Z","end":"2024-04-05T21:22:57.604154Z","steps":["trace[1807872569] 'agreement among raft nodes before linearized reading'  (duration: 300.377741ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-05T21:22:57.604629Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-05T21:22:57.303677Z","time spent":"300.936676ms","remote":"127.0.0.1:57592","response type":"/etcdserverpb.KV/Range","request count":0,"request size":120,"response count":0,"response size":30,"request content":"key:\"/registry/apiextensions.k8s.io/customresourcedefinitions/\" range_end:\"/registry/apiextensions.k8s.io/customresourcedefinitions0\" count_only:true "}
{"level":"info","ts":"2024-04-05T21:23:00.269347Z","caller":"traceutil/trace.go:171","msg":"trace[1631690999] linearizableReadLoop","detail":"{readStateIndex:28320; appliedIndex:28319; }","duration":"338.281659ms","start":"2024-04-05T21:22:59.931047Z","end":"2024-04-05T21:23:00.269328Z","steps":["trace[1631690999] 'read index received'  (duration: 332.331731ms)","trace[1631690999] 'applied index is now lower than readState.Index'  (duration: 5.948806ms)"],"step_count":2}
{"level":"info","ts":"2024-04-05T21:23:00.269348Z","caller":"traceutil/trace.go:171","msg":"trace[1374672869] transaction","detail":"{read_only:false; response_revision:22706; number_of_response:1; }","duration":"457.467412ms","start":"2024-04-05T21:22:59.81187Z","end":"2024-04-05T21:23:00.269337Z","steps":["trace[1374672869] 'process raft request'  (duration: 451.423861ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-05T21:23:00.269456Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"338.410885ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:134"}
{"level":"info","ts":"2024-04-05T21:23:00.269486Z","caller":"traceutil/trace.go:171","msg":"trace[1080680877] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:22706; }","duration":"338.451513ms","start":"2024-04-05T21:22:59.931025Z","end":"2024-04-05T21:23:00.269477Z","steps":["trace[1080680877] 'agreement among raft nodes before linearized reading'  (duration: 338.354077ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-05T21:23:00.269531Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-05T21:22:59.931015Z","time spent":"338.48724ms","remote":"127.0.0.1:57566","response type":"/etcdserverpb.KV/Range","request count":0,"request size":37,"response count":1,"response size":158,"request content":"key:\"/registry/masterleases/192.168.49.2\" "}
{"level":"warn","ts":"2024-04-05T21:23:00.269559Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-05T21:22:59.811855Z","time spent":"457.619282ms","remote":"127.0.0.1:57782","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:22698 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"info","ts":"2024-04-05T21:23:01.129489Z","caller":"traceutil/trace.go:171","msg":"trace[233972916] transaction","detail":"{read_only:false; response_revision:22707; number_of_response:1; }","duration":"853.892055ms","start":"2024-04-05T21:23:00.275579Z","end":"2024-04-05T21:23:01.129471Z","steps":["trace[233972916] 'process raft request'  (duration: 842.969203ms)","trace[233972916] 'compare'  (duration: 10.724399ms)"],"step_count":2}
{"level":"warn","ts":"2024-04-05T21:23:01.129628Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-05T21:23:00.275561Z","time spent":"853.997977ms","remote":"127.0.0.1:57566","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:22699 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128028314643715017 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"info","ts":"2024-04-05T21:23:01.129474Z","caller":"traceutil/trace.go:171","msg":"trace[988250000] transaction","detail":"{read_only:false; response_revision:22708; number_of_response:1; }","duration":"328.98717ms","start":"2024-04-05T21:23:00.80046Z","end":"2024-04-05T21:23:01.129447Z","steps":["trace[988250000] 'process raft request'  (duration: 328.935321ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-05T21:23:01.129871Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-05T21:23:00.800442Z","time spent":"329.321314ms","remote":"127.0.0.1:57782","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:22701 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2024-04-05T21:23:02.030807Z","caller":"traceutil/trace.go:171","msg":"trace[1309086638] transaction","detail":"{read_only:false; response_revision:22709; number_of_response:1; }","duration":"383.00413ms","start":"2024-04-05T21:23:01.647776Z","end":"2024-04-05T21:23:02.03078Z","steps":["trace[1309086638] 'process raft request'  (duration: 382.763046ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-05T21:23:02.031018Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-05T21:23:01.647757Z","time spent":"383.157332ms","remote":"127.0.0.1:57694","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:22705 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-04-05T21:23:05.488403Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"1.447802526s","expected-duration":"1s"}
{"level":"info","ts":"2024-04-05T21:23:05.488809Z","caller":"traceutil/trace.go:171","msg":"trace[1486119356] transaction","detail":"{read_only:false; response_revision:22710; number_of_response:1; }","duration":"1.448250233s","start":"2024-04-05T21:23:04.040537Z","end":"2024-04-05T21:23:05.488787Z","steps":["trace[1486119356] 'process raft request'  (duration: 1.448089637s)"],"step_count":1}
{"level":"warn","ts":"2024-04-05T21:23:05.488978Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-05T21:23:04.040524Z","time spent":"1.448381804s","remote":"127.0.0.1:57694","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:22709 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-04-05T21:23:07.946812Z","caller":"traceutil/trace.go:171","msg":"trace[659786581] transaction","detail":"{read_only:false; response_revision:22711; number_of_response:1; }","duration":"442.572343ms","start":"2024-04-05T21:23:07.504226Z","end":"2024-04-05T21:23:07.946798Z","steps":["trace[659786581] 'process raft request'  (duration: 442.463966ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-05T21:23:07.94694Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-05T21:23:07.504207Z","time spent":"442.671662ms","remote":"127.0.0.1:57694","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:22710 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-04-05T21:30:46.27074Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":22837}
{"level":"info","ts":"2024-04-05T21:30:46.272657Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":22837,"took":"1.68524ms","hash":3309985691}
{"level":"info","ts":"2024-04-05T21:30:46.272726Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3309985691,"revision":22837,"compact-revision":22153}
{"level":"info","ts":"2024-04-05T21:35:46.278027Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":23076}
{"level":"info","ts":"2024-04-05T21:35:46.279059Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":23076,"took":"723.969µs","hash":2862900642}
{"level":"info","ts":"2024-04-05T21:35:46.279121Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2862900642,"revision":23076,"compact-revision":22837}
{"level":"info","ts":"2024-04-05T21:40:46.30068Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":23316}
{"level":"info","ts":"2024-04-05T21:40:46.30169Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":23316,"took":"800.485µs","hash":4187884699}
{"level":"info","ts":"2024-04-05T21:40:46.301762Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4187884699,"revision":23316,"compact-revision":23076}
{"level":"info","ts":"2024-04-05T21:45:46.30779Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":23557}
{"level":"info","ts":"2024-04-05T21:45:46.308876Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":23557,"took":"845.651µs","hash":1125466578}
{"level":"info","ts":"2024-04-05T21:45:46.308953Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1125466578,"revision":23557,"compact-revision":23316}
{"level":"info","ts":"2024-04-05T21:50:20.022142Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":30003,"local-member-snapshot-index":20002,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-04-05T21:50:20.03168Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":30003}
{"level":"info","ts":"2024-04-05T21:50:20.031939Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":25003}
{"level":"info","ts":"2024-04-05T21:50:46.316884Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":23796}
{"level":"info","ts":"2024-04-05T21:50:46.318467Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":23796,"took":"1.384092ms","hash":4101552785}
{"level":"info","ts":"2024-04-05T21:50:46.318525Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4101552785,"revision":23796,"compact-revision":23557}
{"level":"info","ts":"2024-04-05T21:54:01.015689Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-04-05T21:54:01.015748Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-04-05T21:54:01.015823Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-04-05T21:54:01.015886Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-04-05T21:54:01.124345Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-04-05T21:54:01.124423Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-04-05T21:54:01.124474Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-04-05T21:54:01.128063Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-05T21:54:01.128134Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-05T21:54:01.128142Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [6c3fcf67d556] <==
{"level":"info","ts":"2024-04-05T22:08:45.024344Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-04-05T22:08:45.02614Z","caller":"auth/store.go:1238","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-04-05T22:08:45.027901Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":23796}
{"level":"info","ts":"2024-04-05T22:08:45.031175Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":24240}
{"level":"info","ts":"2024-04-05T22:08:45.034004Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-04-05T22:08:45.036942Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-04-05T22:08:45.037418Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-04-05T22:08:45.037497Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.9","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2024-04-05T22:08:45.037718Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-04-05T22:08:45.037792Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-04-05T22:08:45.037825Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-04-05T22:08:45.037834Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-04-05T22:08:45.0397Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-04-05T22:08:45.039885Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-04-05T22:08:45.039907Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-04-05T22:08:45.039986Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-05T22:08:45.039993Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-05T22:08:45.92469Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 5"}
{"level":"info","ts":"2024-04-05T22:08:45.924857Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 5"}
{"level":"info","ts":"2024-04-05T22:08:45.924885Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2024-04-05T22:08:45.924904Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 6"}
{"level":"info","ts":"2024-04-05T22:08:45.924922Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 6"}
{"level":"info","ts":"2024-04-05T22:08:45.924937Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 6"}
{"level":"info","ts":"2024-04-05T22:08:45.924983Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 6"}
{"level":"info","ts":"2024-04-05T22:08:45.929814Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-04-05T22:08:45.929852Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-04-05T22:08:45.929858Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-04-05T22:08:45.930752Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-04-05T22:08:45.930832Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-04-05T22:08:45.931159Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-04-05T22:08:45.931164Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-04-05T22:13:43.961881Z","caller":"traceutil/trace.go:171","msg":"trace[1545062940] linearizableReadLoop","detail":"{readStateIndex:30649; appliedIndex:30648; }","duration":"103.332493ms","start":"2024-04-05T22:13:43.858529Z","end":"2024-04-05T22:13:43.961862Z","steps":["trace[1545062940] 'read index received'  (duration: 103.207048ms)","trace[1545062940] 'applied index is now lower than readState.Index'  (duration: 124.734µs)"],"step_count":2}
{"level":"info","ts":"2024-04-05T22:13:43.962018Z","caller":"traceutil/trace.go:171","msg":"trace[1482646857] transaction","detail":"{read_only:false; response_revision:24575; number_of_response:1; }","duration":"128.882612ms","start":"2024-04-05T22:13:43.833117Z","end":"2024-04-05T22:13:43.962Z","steps":["trace[1482646857] 'process raft request'  (duration: 128.519576ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-05T22:13:43.962231Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"103.697698ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1111"}
{"level":"info","ts":"2024-04-05T22:13:43.96242Z","caller":"traceutil/trace.go:171","msg":"trace[1347678944] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:24575; }","duration":"103.89006ms","start":"2024-04-05T22:13:43.858506Z","end":"2024-04-05T22:13:43.962396Z","steps":["trace[1347678944] 'agreement among raft nodes before linearized reading'  (duration: 103.564665ms)"],"step_count":1}
{"level":"info","ts":"2024-04-05T22:13:56.229374Z","caller":"traceutil/trace.go:171","msg":"trace[370538738] transaction","detail":"{read_only:false; response_revision:24587; number_of_response:1; }","duration":"173.749876ms","start":"2024-04-05T22:13:56.055606Z","end":"2024-04-05T22:13:56.229356Z","steps":["trace[370538738] 'process raft request'  (duration: 173.608817ms)"],"step_count":1}
{"level":"info","ts":"2024-04-05T22:14:22.500797Z","caller":"traceutil/trace.go:171","msg":"trace[1207225147] transaction","detail":"{read_only:false; response_revision:24613; number_of_response:1; }","duration":"110.107932ms","start":"2024-04-05T22:14:22.390665Z","end":"2024-04-05T22:14:22.500773Z","steps":["trace[1207225147] 'process raft request'  (duration: 109.934883ms)"],"step_count":1}
{"level":"info","ts":"2024-04-05T22:14:50.843139Z","caller":"traceutil/trace.go:171","msg":"trace[1924354023] transaction","detail":"{read_only:false; response_revision:24639; number_of_response:1; }","duration":"134.931379ms","start":"2024-04-05T22:14:50.708154Z","end":"2024-04-05T22:14:50.843085Z","steps":["trace[1924354023] 'process raft request'  (duration: 134.776604ms)"],"step_count":1}
{"level":"info","ts":"2024-04-05T22:14:54.984042Z","caller":"traceutil/trace.go:171","msg":"trace[1747147977] transaction","detail":"{read_only:false; response_revision:24641; number_of_response:1; }","duration":"118.300601ms","start":"2024-04-05T22:14:54.865713Z","end":"2024-04-05T22:14:54.984013Z","steps":["trace[1747147977] 'process raft request'  (duration: 118.017737ms)"],"step_count":1}
{"level":"info","ts":"2024-04-05T22:14:59.138325Z","caller":"traceutil/trace.go:171","msg":"trace[2023879823] transaction","detail":"{read_only:false; response_revision:24645; number_of_response:1; }","duration":"134.629661ms","start":"2024-04-05T22:14:59.003677Z","end":"2024-04-05T22:14:59.138307Z","steps":["trace[2023879823] 'process raft request'  (duration: 134.464276ms)"],"step_count":1}
{"level":"info","ts":"2024-04-05T22:15:05.749357Z","caller":"traceutil/trace.go:171","msg":"trace[239540318] transaction","detail":"{read_only:false; response_revision:24651; number_of_response:1; }","duration":"122.302ms","start":"2024-04-05T22:15:05.627028Z","end":"2024-04-05T22:15:05.74933Z","steps":["trace[239540318] 'process raft request'  (duration: 122.157074ms)"],"step_count":1}
{"level":"info","ts":"2024-04-05T22:15:20.587411Z","caller":"traceutil/trace.go:171","msg":"trace[1806381359] transaction","detail":"{read_only:false; response_revision:24662; number_of_response:1; }","duration":"134.106627ms","start":"2024-04-05T22:15:20.453288Z","end":"2024-04-05T22:15:20.587395Z","steps":["trace[1806381359] 'process raft request'  (duration: 133.001206ms)"],"step_count":1}
{"level":"info","ts":"2024-04-05T22:15:23.421599Z","caller":"traceutil/trace.go:171","msg":"trace[1880768023] transaction","detail":"{read_only:false; response_revision:24664; number_of_response:1; }","duration":"134.762722ms","start":"2024-04-05T22:15:23.28682Z","end":"2024-04-05T22:15:23.421583Z","steps":["trace[1880768023] 'process raft request'  (duration: 134.662671ms)"],"step_count":1}
{"level":"info","ts":"2024-04-05T22:15:25.557643Z","caller":"traceutil/trace.go:171","msg":"trace[2022230003] transaction","detail":"{read_only:false; response_revision:24665; number_of_response:1; }","duration":"126.757569ms","start":"2024-04-05T22:15:25.430838Z","end":"2024-04-05T22:15:25.557596Z","steps":["trace[2022230003] 'process raft request'  (duration: 126.192554ms)"],"step_count":1}
{"level":"info","ts":"2024-04-05T22:15:33.830541Z","caller":"traceutil/trace.go:171","msg":"trace[649729876] transaction","detail":"{read_only:false; response_revision:24672; number_of_response:1; }","duration":"135.792666ms","start":"2024-04-05T22:15:33.694733Z","end":"2024-04-05T22:15:33.830526Z","steps":["trace[649729876] 'process raft request'  (duration: 135.651267ms)"],"step_count":1}
{"level":"info","ts":"2024-04-05T22:15:36.512026Z","caller":"traceutil/trace.go:171","msg":"trace[1451767022] transaction","detail":"{read_only:false; response_revision:24674; number_of_response:1; }","duration":"134.54078ms","start":"2024-04-05T22:15:36.377468Z","end":"2024-04-05T22:15:36.512009Z","steps":["trace[1451767022] 'process raft request'  (duration: 134.381446ms)"],"step_count":1}
{"level":"info","ts":"2024-04-05T22:16:08.277061Z","caller":"traceutil/trace.go:171","msg":"trace[910787860] transaction","detail":"{read_only:false; response_revision:24700; number_of_response:1; }","duration":"130.542613ms","start":"2024-04-05T22:16:08.146499Z","end":"2024-04-05T22:16:08.277041Z","steps":["trace[910787860] 'process raft request'  (duration: 130.365861ms)"],"step_count":1}
{"level":"info","ts":"2024-04-05T22:16:17.301163Z","caller":"traceutil/trace.go:171","msg":"trace[795781812] transaction","detail":"{read_only:false; response_revision:24706; number_of_response:1; }","duration":"132.470854ms","start":"2024-04-05T22:16:17.16867Z","end":"2024-04-05T22:16:17.301141Z","steps":["trace[795781812] 'process raft request'  (duration: 132.333733ms)"],"step_count":1}
{"level":"info","ts":"2024-04-05T22:18:46.005278Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":24579}
{"level":"info","ts":"2024-04-05T22:18:46.02159Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":24579,"took":"16.075742ms","hash":3578350623}
{"level":"info","ts":"2024-04-05T22:18:46.021702Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3578350623,"revision":24579,"compact-revision":23796}
{"level":"info","ts":"2024-04-05T22:23:46.012954Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":24825}
{"level":"info","ts":"2024-04-05T22:23:46.013897Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":24825,"took":"752.624µs","hash":1687496406}
{"level":"info","ts":"2024-04-05T22:23:46.013968Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1687496406,"revision":24825,"compact-revision":24579}
{"level":"info","ts":"2024-04-05T22:28:46.020146Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":25109}
{"level":"info","ts":"2024-04-05T22:28:46.021284Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":25109,"took":"852.504µs","hash":2790657655}
{"level":"info","ts":"2024-04-05T22:28:46.021364Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2790657655,"revision":25109,"compact-revision":24825}
{"level":"info","ts":"2024-04-05T22:33:46.027753Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":25397}
{"level":"info","ts":"2024-04-05T22:33:46.029099Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":25397,"took":"1.028991ms","hash":4264682384}
{"level":"info","ts":"2024-04-05T22:33:46.029177Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4264682384,"revision":25397,"compact-revision":25109}


==> kernel <==
 22:36:12 up  1:16,  0 users,  load average: 0.31, 0.42, 0.34
Linux minikube 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"


==> kube-apiserver [38557dfd27ea] <==
I0405 22:36:12.773525       1 apf_controller.go:1057] plState.quiescing=false, plState.numPending=0, useless=false
I0405 22:36:12.773614       1 httplog.go:132] "HTTP" verb="LIST" URI="/api/v1/events?fieldSelector=involvedObject.name%!D(MISSING)minikube%!C(MISSING)involvedObject.namespace%!D(MISSING)%!C(MISSING)involvedObject.kind%!D(MISSING)Node%!C(MISSING)involvedObject.uid%!D(MISSING)1f55640d-14fe-48dc-9d87-90a78e664e88&limit=500" latency="5.497889ms" userAgent="kubectl/v1.28.3 (linux/amd64) kubernetes/a8a1abc" audit-ID="1f7969e2-6406-44f2-9b5e-c4941d9fcac0" srcIP="127.0.0.1:42288" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="4.406369ms" resp=200
I0405 22:36:12.775342       1 apf_controller.go:989] startRequest(RequestDigest{RequestInfo: &request.RequestInfo{IsResourceRequest:true, Path:"/api/v1/events", Verb:"list", APIPrefix:"api", APIGroup:"", APIVersion:"v1", Namespace:"", Resource:"events", Subresource:"", Name:"", Parts:[]string{"events"}}, User: &user.DefaultInfo{Name:"minikube", UID:"", Groups:[]string{"system:masters", "system:authenticated"}, Extra:map[string][]string(nil)}})
I0405 22:36:12.775421       1 apf_controller.go:1037] startRequest(RequestDigest{RequestInfo: &request.RequestInfo{IsResourceRequest:true, Path:"/api/v1/events", Verb:"list", APIPrefix:"api", APIGroup:"", APIVersion:"v1", Namespace:"", Resource:"events", Subresource:"", Name:"", Parts:[]string{"events"}}, User: &user.DefaultInfo{Name:"minikube", UID:"", Groups:[]string{"system:masters", "system:authenticated"}, Extra:map[string][]string(nil)}}) => fsName="exempt", distMethod=(*v1beta3.FlowDistinguisherMethod)(nil), plName="exempt", numQueues=0
I0405 22:36:12.775468       1 queueset.go:742] QS(exempt) at t=2024-04-05 22:36:12.775446465 R=0.00000000ss: immediate dispatch of request "exempt" &request.RequestInfo{IsResourceRequest:true, Path:"/api/v1/events", Verb:"list", APIPrefix:"api", APIGroup:"", APIVersion:"v1", Namespace:"", Resource:"events", Subresource:"", Name:"", Parts:[]string{"events"}} &user.DefaultInfo{Name:"minikube", UID:"", Groups:[]string{"system:masters", "system:authenticated"}, Extra:map[string][]string(nil)}, qs will have 1 executing
I0405 22:36:12.775486       1 apf_filter.go:174] Handle(RequestDigest{RequestInfo: &request.RequestInfo{IsResourceRequest:true, Path:"/api/v1/events", Verb:"list", APIPrefix:"api", APIGroup:"", APIVersion:"v1", Namespace:"", Resource:"events", Subresource:"", Name:"", Parts:[]string{"events"}}, User: &user.DefaultInfo{Name:"minikube", UID:"", Groups:[]string{"system:masters", "system:authenticated"}, Extra:map[string][]string(nil)}}) => fsName="exempt", distMethod=(*v1beta3.FlowDistinguisherMethod)(nil), plName="exempt", isExempt=true, queued=false
I0405 22:36:12.775534       1 queueset.go:433] QS(exempt): Dispatching request &request.RequestInfo{IsResourceRequest:true, Path:"/api/v1/events", Verb:"list", APIPrefix:"api", APIGroup:"", APIVersion:"v1", Namespace:"", Resource:"events", Subresource:"", Name:"", Parts:[]string{"events"}} &user.DefaultInfo{Name:"minikube", UID:"", Groups:[]string{"system:masters", "system:authenticated"}, Extra:map[string][]string(nil)} from its queue
I0405 22:36:12.775572       1 handler.go:153] kube-aggregator: GET "/api/v1/events" satisfied by nonGoRestful
I0405 22:36:12.775609       1 pathrecorder.go:248] kube-aggregator: "/api/v1/events" satisfied by prefix /api/
I0405 22:36:12.775620       1 handler.go:143] kube-apiserver: GET "/api/v1/events" satisfied by gorestful with webservice /api/v1
I0405 22:36:12.780093       1 queueset.go:967] QS(exempt) at t=2024-04-05 22:36:12.780067924 R=0.00000000ss: request &request.RequestInfo{IsResourceRequest:true, Path:"/api/v1/events", Verb:"list", APIPrefix:"api", APIGroup:"", APIVersion:"v1", Namespace:"", Resource:"events", Subresource:"", Name:"", Parts:[]string{"events"}} &user.DefaultInfo{Name:"minikube", UID:"", Groups:[]string{"system:masters", "system:authenticated"}, Extra:map[string][]string(nil)} finished all use of 1 seats, qs will have 0 requests occupying 0 seats
I0405 22:36:12.780171       1 apf_filter.go:178] Handle(RequestDigest{RequestInfo: &request.RequestInfo{IsResourceRequest:true, Path:"/api/v1/events", Verb:"list", APIPrefix:"api", APIGroup:"", APIVersion:"v1", Namespace:"", Resource:"events", Subresource:"", Name:"", Parts:[]string{"events"}}, User: &user.DefaultInfo{Name:"minikube", UID:"", Groups:[]string{"system:masters", "system:authenticated"}, Extra:map[string][]string(nil)}}) => fsName="exempt", distMethod=(*v1beta3.FlowDistinguisherMethod)(nil), plName="exempt", isExempt=true, queued=false, Finish() => panicking=false idle=true
I0405 22:36:12.780213       1 apf_controller.go:1057] plState.quiescing=false, plState.numPending=0, useless=false
I0405 22:36:12.780332       1 httplog.go:132] "HTTP" verb="LIST" URI="/api/v1/events?fieldSelector=involvedObject.name%!D(MISSING)minikube%!C(MISSING)involvedObject.namespace%!D(MISSING)%!C(MISSING)involvedObject.kind%!D(MISSING)Node%!C(MISSING)involvedObject.uid%!D(MISSING)minikube&limit=500" latency="5.524089ms" userAgent="kubectl/v1.28.3 (linux/amd64) kubernetes/a8a1abc" audit-ID="d66cc95d-fea7-41d0-b0c8-cf1ae6adcb1d" srcIP="127.0.0.1:42288" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="4.510247ms" resp=200
I0405 22:36:12.783225       1 apf_controller.go:989] startRequest(RequestDigest{RequestInfo: &request.RequestInfo{IsResourceRequest:true, Path:"/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube", Verb:"get", APIPrefix:"apis", APIGroup:"coordination.k8s.io", APIVersion:"v1", Namespace:"kube-node-lease", Resource:"leases", Subresource:"", Name:"minikube", Parts:[]string{"leases", "minikube"}}, User: &user.DefaultInfo{Name:"minikube", UID:"", Groups:[]string{"system:masters", "system:authenticated"}, Extra:map[string][]string(nil)}})
I0405 22:36:12.783315       1 apf_controller.go:1037] startRequest(RequestDigest{RequestInfo: &request.RequestInfo{IsResourceRequest:true, Path:"/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube", Verb:"get", APIPrefix:"apis", APIGroup:"coordination.k8s.io", APIVersion:"v1", Namespace:"kube-node-lease", Resource:"leases", Subresource:"", Name:"minikube", Parts:[]string{"leases", "minikube"}}, User: &user.DefaultInfo{Name:"minikube", UID:"", Groups:[]string{"system:masters", "system:authenticated"}, Extra:map[string][]string(nil)}}) => fsName="exempt", distMethod=(*v1beta3.FlowDistinguisherMethod)(nil), plName="exempt", numQueues=0
I0405 22:36:12.783508       1 queueset.go:742] QS(exempt) at t=2024-04-05 22:36:12.783342654 R=0.00000000ss: immediate dispatch of request "exempt" &request.RequestInfo{IsResourceRequest:true, Path:"/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube", Verb:"get", APIPrefix:"apis", APIGroup:"coordination.k8s.io", APIVersion:"v1", Namespace:"kube-node-lease", Resource:"leases", Subresource:"", Name:"minikube", Parts:[]string{"leases", "minikube"}} &user.DefaultInfo{Name:"minikube", UID:"", Groups:[]string{"system:masters", "system:authenticated"}, Extra:map[string][]string(nil)}, qs will have 1 executing
I0405 22:36:12.783584       1 apf_filter.go:174] Handle(RequestDigest{RequestInfo: &request.RequestInfo{IsResourceRequest:true, Path:"/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube", Verb:"get", APIPrefix:"apis", APIGroup:"coordination.k8s.io", APIVersion:"v1", Namespace:"kube-node-lease", Resource:"leases", Subresource:"", Name:"minikube", Parts:[]string{"leases", "minikube"}}, User: &user.DefaultInfo{Name:"minikube", UID:"", Groups:[]string{"system:masters", "system:authenticated"}, Extra:map[string][]string(nil)}}) => fsName="exempt", distMethod=(*v1beta3.FlowDistinguisherMethod)(nil), plName="exempt", isExempt=true, queued=false
I0405 22:36:12.783622       1 queueset.go:433] QS(exempt): Dispatching request &request.RequestInfo{IsResourceRequest:true, Path:"/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube", Verb:"get", APIPrefix:"apis", APIGroup:"coordination.k8s.io", APIVersion:"v1", Namespace:"kube-node-lease", Resource:"leases", Subresource:"", Name:"minikube", Parts:[]string{"leases", "minikube"}} &user.DefaultInfo{Name:"minikube", UID:"", Groups:[]string{"system:masters", "system:authenticated"}, Extra:map[string][]string(nil)} from its queue
I0405 22:36:12.783735       1 handler.go:153] kube-aggregator: GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" satisfied by nonGoRestful
I0405 22:36:12.783756       1 pathrecorder.go:248] kube-aggregator: "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" satisfied by prefix /apis/coordination.k8s.io/v1/
I0405 22:36:12.783821       1 handler.go:143] kube-apiserver: GET "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" satisfied by gorestful with webservice /apis/coordination.k8s.io/v1
I0405 22:36:12.785498       1 queueset.go:967] QS(exempt) at t=2024-04-05 22:36:12.785460562 R=0.00000000ss: request &request.RequestInfo{IsResourceRequest:true, Path:"/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube", Verb:"get", APIPrefix:"apis", APIGroup:"coordination.k8s.io", APIVersion:"v1", Namespace:"kube-node-lease", Resource:"leases", Subresource:"", Name:"minikube", Parts:[]string{"leases", "minikube"}} &user.DefaultInfo{Name:"minikube", UID:"", Groups:[]string{"system:masters", "system:authenticated"}, Extra:map[string][]string(nil)} finished all use of 1 seats, qs will have 0 requests occupying 0 seats
I0405 22:36:12.785584       1 apf_filter.go:178] Handle(RequestDigest{RequestInfo: &request.RequestInfo{IsResourceRequest:true, Path:"/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube", Verb:"get", APIPrefix:"apis", APIGroup:"coordination.k8s.io", APIVersion:"v1", Namespace:"kube-node-lease", Resource:"leases", Subresource:"", Name:"minikube", Parts:[]string{"leases", "minikube"}}, User: &user.DefaultInfo{Name:"minikube", UID:"", Groups:[]string{"system:masters", "system:authenticated"}, Extra:map[string][]string(nil)}}) => fsName="exempt", distMethod=(*v1beta3.FlowDistinguisherMethod)(nil), plName="exempt", isExempt=true, queued=false, Finish() => panicking=false idle=true
I0405 22:36:12.785610       1 apf_controller.go:1057] plState.quiescing=false, plState.numPending=0, useless=false
I0405 22:36:12.785697       1 httplog.go:132] "HTTP" verb="GET" URI="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" latency="3.032761ms" userAgent="kubectl/v1.28.3 (linux/amd64) kubernetes/a8a1abc" audit-ID="2f361fe1-4474-427b-8e58-a55eb63f3754" srcIP="127.0.0.1:42288" apf_pl="exempt" apf_fs="exempt" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.762952ms" resp=200
I0405 22:36:12.901159       1 apf_controller.go:989] startRequest(RequestDigest{RequestInfo: &request.RequestInfo{IsResourceRequest:false, Path:"/readyz", Verb:"get", APIPrefix:"", APIGroup:"", APIVersion:"", Namespace:"", Resource:"", Subresource:"", Name:"", Parts:[]string(nil)}, User: &user.DefaultInfo{Name:"system:anonymous", UID:"", Groups:[]string{"system:unauthenticated"}, Extra:map[string][]string(nil)}})
I0405 22:36:12.901262       1 apf_controller.go:1037] startRequest(RequestDigest{RequestInfo: &request.RequestInfo{IsResourceRequest:false, Path:"/readyz", Verb:"get", APIPrefix:"", APIGroup:"", APIVersion:"", Namespace:"", Resource:"", Subresource:"", Name:"", Parts:[]string(nil)}, User: &user.DefaultInfo{Name:"system:anonymous", UID:"", Groups:[]string{"system:unauthenticated"}, Extra:map[string][]string(nil)}}) => fsName="probes", distMethod=(*v1beta3.FlowDistinguisherMethod)(nil), plName="exempt", numQueues=0
I0405 22:36:12.901381       1 queueset.go:742] QS(exempt) at t=2024-04-05 22:36:12.901341095 R=0.00000000ss: immediate dispatch of request "probes" &request.RequestInfo{IsResourceRequest:false, Path:"/readyz", Verb:"get", APIPrefix:"", APIGroup:"", APIVersion:"", Namespace:"", Resource:"", Subresource:"", Name:"", Parts:[]string(nil)} &user.DefaultInfo{Name:"system:anonymous", UID:"", Groups:[]string{"system:unauthenticated"}, Extra:map[string][]string(nil)}, qs will have 1 executing
I0405 22:36:12.901441       1 apf_filter.go:174] Handle(RequestDigest{RequestInfo: &request.RequestInfo{IsResourceRequest:false, Path:"/readyz", Verb:"get", APIPrefix:"", APIGroup:"", APIVersion:"", Namespace:"", Resource:"", Subresource:"", Name:"", Parts:[]string(nil)}, User: &user.DefaultInfo{Name:"system:anonymous", UID:"", Groups:[]string{"system:unauthenticated"}, Extra:map[string][]string(nil)}}) => fsName="probes", distMethod=(*v1beta3.FlowDistinguisherMethod)(nil), plName="exempt", isExempt=true, queued=false
I0405 22:36:12.901523       1 queueset.go:433] QS(exempt): Dispatching request &request.RequestInfo{IsResourceRequest:false, Path:"/readyz", Verb:"get", APIPrefix:"", APIGroup:"", APIVersion:"", Namespace:"", Resource:"", Subresource:"", Name:"", Parts:[]string(nil)} &user.DefaultInfo{Name:"system:anonymous", UID:"", Groups:[]string{"system:unauthenticated"}, Extra:map[string][]string(nil)} from its queue
I0405 22:36:12.901602       1 handler.go:153] kube-aggregator: GET "/readyz" satisfied by nonGoRestful
I0405 22:36:12.901616       1 pathrecorder.go:241] kube-aggregator: "/readyz" satisfied by exact match
I0405 22:36:12.904042       1 shared_informer.go:341] caches populated
I0405 22:36:12.904100       1 shared_informer.go:341] caches populated
I0405 22:36:12.904106       1 shared_informer.go:341] caches populated
I0405 22:36:12.904110       1 shared_informer.go:341] caches populated
I0405 22:36:12.904113       1 shared_informer.go:341] caches populated
I0405 22:36:12.904117       1 shared_informer.go:341] caches populated
I0405 22:36:12.904121       1 shared_informer.go:341] caches populated
I0405 22:36:12.904125       1 shared_informer.go:341] caches populated
I0405 22:36:12.904128       1 shared_informer.go:341] caches populated
I0405 22:36:12.904133       1 shared_informer.go:341] caches populated
I0405 22:36:12.904137       1 shared_informer.go:341] caches populated
I0405 22:36:12.904140       1 shared_informer.go:341] caches populated
I0405 22:36:12.904143       1 shared_informer.go:341] caches populated
I0405 22:36:12.904147       1 shared_informer.go:341] caches populated
I0405 22:36:12.904155       1 shared_informer.go:341] caches populated
I0405 22:36:12.904162       1 shared_informer.go:341] caches populated
I0405 22:36:12.904167       1 shared_informer.go:341] caches populated
I0405 22:36:12.904177       1 shared_informer.go:341] caches populated
I0405 22:36:12.904186       1 shared_informer.go:341] caches populated
I0405 22:36:12.904191       1 shared_informer.go:341] caches populated
I0405 22:36:12.904195       1 shared_informer.go:341] caches populated
I0405 22:36:12.904200       1 shared_informer.go:341] caches populated
I0405 22:36:12.904205       1 shared_informer.go:341] caches populated
I0405 22:36:12.904391       1 queueset.go:967] QS(exempt) at t=2024-04-05 22:36:12.904368692 R=0.00000000ss: request &request.RequestInfo{IsResourceRequest:false, Path:"/readyz", Verb:"get", APIPrefix:"", APIGroup:"", APIVersion:"", Namespace:"", Resource:"", Subresource:"", Name:"", Parts:[]string(nil)} &user.DefaultInfo{Name:"system:anonymous", UID:"", Groups:[]string{"system:unauthenticated"}, Extra:map[string][]string(nil)} finished all use of 1 seats, qs will have 0 requests occupying 0 seats
I0405 22:36:12.904472       1 apf_filter.go:178] Handle(RequestDigest{RequestInfo: &request.RequestInfo{IsResourceRequest:false, Path:"/readyz", Verb:"get", APIPrefix:"", APIGroup:"", APIVersion:"", Namespace:"", Resource:"", Subresource:"", Name:"", Parts:[]string(nil)}, User: &user.DefaultInfo{Name:"system:anonymous", UID:"", Groups:[]string{"system:unauthenticated"}, Extra:map[string][]string(nil)}}) => fsName="probes", distMethod=(*v1beta3.FlowDistinguisherMethod)(nil), plName="exempt", isExempt=true, queued=false, Finish() => panicking=false idle=true
I0405 22:36:12.904512       1 apf_controller.go:1057] plState.quiescing=false, plState.numPending=0, useless=false
I0405 22:36:12.904624       1 httplog.go:132] "HTTP" verb="GET" URI="/readyz" latency="3.612853ms" userAgent="kube-probe/1.28" audit-ID="923ddf9b-eb60-4ec3-8db4-e16f7868592d" srcIP="192.168.49.2:34726" apf_pl="exempt" apf_fs="probes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="2.820172ms" resp=200


==> kube-apiserver [fcc06fdf610b] <==
I0405 21:54:10.893812       1 balancer.go:183] "[balancer] base.baseBalancer: handle SubConn state change: 0xc002c32228, CONNECTING\n"
I0405 21:54:10.893860       1 clientconn.go:1288] "[core] Creating new client transport to \"{\\n  \\\"Addr\\\": \\\"127.0.0.1:2379\\\",\\n  \\\"ServerName\\\": \\\"127.0.0.1\\\",\\n  \\\"Attributes\\\": null,\\n  \\\"BalancerAttributes\\\": null,\\n  \\\"Type\\\": 0,\\n  \\\"Metadata\\\": null\\n}\": connection error: desc = \"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\"\n"
W0405 21:54:10.893878       1 logging.go:59] [core] [Channel #136 SubChannel #137] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
I0405 21:54:10.893933       1 logging.go:43] "[core] [Channel #136 SubChannel #137] Subchannel Connectivity change to TRANSIENT_FAILURE, last error: connection error: desc = \"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\"\n"
I0405 21:54:10.893963       1 balancer.go:183] "[balancer] base.baseBalancer: handle SubConn state change: 0xc002c32228, TRANSIENT_FAILURE\n"
I0405 21:54:10.904524       1 logging.go:43] "[core] [Channel #109 SubChannel #110] Subchannel Connectivity change to IDLE, last error: connection error: desc = \"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\"\n"
I0405 21:54:10.904617       1 balancer.go:183] "[balancer] base.baseBalancer: handle SubConn state change: 0xc0015238c0, IDLE\n"
I0405 21:54:10.904664       1 logging.go:43] "[core] [Channel #109 SubChannel #110] Subchannel Connectivity change to CONNECTING\n"
I0405 21:54:10.904678       1 logging.go:43] "[core] [Channel #109 SubChannel #110] Subchannel picks a new address \"127.0.0.1:2379\" to connect\n"
I0405 21:54:10.904823       1 balancer.go:183] "[balancer] base.baseBalancer: handle SubConn state change: 0xc0015238c0, CONNECTING\n"
I0405 21:54:10.904975       1 clientconn.go:1288] "[core] Creating new client transport to \"{\\n  \\\"Addr\\\": \\\"127.0.0.1:2379\\\",\\n  \\\"ServerName\\\": \\\"127.0.0.1\\\",\\n  \\\"Attributes\\\": null,\\n  \\\"BalancerAttributes\\\": null,\\n  \\\"Type\\\": 0,\\n  \\\"Metadata\\\": null\\n}\": connection error: desc = \"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\"\n"
W0405 21:54:10.905034       1 logging.go:59] [core] [Channel #109 SubChannel #110] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
I0405 21:54:10.905092       1 logging.go:43] "[core] [Channel #109 SubChannel #110] Subchannel Connectivity change to TRANSIENT_FAILURE, last error: connection error: desc = \"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\"\n"
I0405 21:54:10.905127       1 balancer.go:183] "[balancer] base.baseBalancer: handle SubConn state change: 0xc0015238c0, TRANSIENT_FAILURE\n"
I0405 21:54:10.980851       1 logging.go:43] "[core] [Channel #49 SubChannel #50] Subchannel Connectivity change to IDLE, last error: connection error: desc = \"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\"\n"
I0405 21:54:10.980937       1 balancer.go:183] "[balancer] base.baseBalancer: handle SubConn state change: 0xc0018f3290, IDLE\n"
I0405 21:54:10.980994       1 logging.go:43] "[core] [Channel #49 SubChannel #50] Subchannel Connectivity change to CONNECTING\n"
I0405 21:54:10.981024       1 logging.go:43] "[core] [Channel #49 SubChannel #50] Subchannel picks a new address \"127.0.0.1:2379\" to connect\n"
I0405 21:54:10.981112       1 balancer.go:183] "[balancer] base.baseBalancer: handle SubConn state change: 0xc0018f3290, CONNECTING\n"
I0405 21:54:10.981349       1 clientconn.go:1288] "[core] Creating new client transport to \"{\\n  \\\"Addr\\\": \\\"127.0.0.1:2379\\\",\\n  \\\"ServerName\\\": \\\"127.0.0.1\\\",\\n  \\\"Attributes\\\": null,\\n  \\\"BalancerAttributes\\\": null,\\n  \\\"Type\\\": 0,\\n  \\\"Metadata\\\": null\\n}\": connection error: desc = \"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\"\n"
W0405 21:54:10.981400       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
I0405 21:54:10.981449       1 logging.go:43] "[core] [Channel #49 SubChannel #50] Subchannel Connectivity change to TRANSIENT_FAILURE, last error: connection error: desc = \"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\"\n"
I0405 21:54:10.981469       1 balancer.go:183] "[balancer] base.baseBalancer: handle SubConn state change: 0xc0018f3290, TRANSIENT_FAILURE\n"
I0405 21:54:11.011538       1 logging.go:43] "[core] [Channel #154 SubChannel #155] Subchannel Connectivity change to IDLE, last error: connection error: desc = \"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\"\n"
I0405 21:54:11.011630       1 balancer.go:183] "[balancer] base.baseBalancer: handle SubConn state change: 0xc002be8ac8, IDLE\n"
I0405 21:54:11.011645       1 logging.go:43] "[core] [Channel #154 SubChannel #155] Subchannel Connectivity change to CONNECTING\n"
I0405 21:54:11.011657       1 logging.go:43] "[core] [Channel #154 SubChannel #155] Subchannel picks a new address \"127.0.0.1:2379\" to connect\n"
I0405 21:54:11.011786       1 balancer.go:183] "[balancer] base.baseBalancer: handle SubConn state change: 0xc002be8ac8, CONNECTING\n"
I0405 21:54:11.011932       1 clientconn.go:1288] "[core] Creating new client transport to \"{\\n  \\\"Addr\\\": \\\"127.0.0.1:2379\\\",\\n  \\\"ServerName\\\": \\\"127.0.0.1\\\",\\n  \\\"Attributes\\\": null,\\n  \\\"BalancerAttributes\\\": null,\\n  \\\"Type\\\": 0,\\n  \\\"Metadata\\\": null\\n}\": connection error: desc = \"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\"\n"
W0405 21:54:11.011987       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
I0405 21:54:11.012042       1 logging.go:43] "[core] [Channel #154 SubChannel #155] Subchannel Connectivity change to TRANSIENT_FAILURE, last error: connection error: desc = \"transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused\"\n"
I0405 21:54:11.012072       1 balancer.go:183] "[balancer] base.baseBalancer: handle SubConn state change: 0xc002be8ac8, TRANSIENT_FAILURE\n"


==> kube-controller-manager [9fdad6b3b0e8] <==
I0405 22:22:09.494927       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="11.684597ms"
I0405 22:22:09.495058       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="73.119µs"
I0405 22:22:09.505527       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="110.46µs"
I0405 22:22:13.797484       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="68.51µs"
I0405 22:22:24.830749       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="99.099µs"
I0405 22:22:38.827534       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="106.773µs"
I0405 22:22:51.825336       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="138.143µs"
I0405 22:23:06.828930       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="53.241µs"
I0405 22:23:19.827203       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="74.121µs"
I0405 22:23:58.827234       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="95.271µs"
I0405 22:24:12.826417       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="80.714µs"
I0405 22:25:38.825488       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="113.055µs"
I0405 22:25:50.825442       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="89.24µs"
I0405 22:28:06.675245       1 event.go:307] "Event occurred" object="default/second-image-6bbb48bfb7" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: second-image-6bbb48bfb7-shx9c"
I0405 22:28:06.687282       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="33.170301ms"
I0405 22:28:06.696392       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="9.031987ms"
I0405 22:28:06.696544       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="61.237µs"
I0405 22:28:06.698899       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="82.337µs"
I0405 22:28:06.901039       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="49.654µs"
I0405 22:28:07.380813       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="80.553µs"
I0405 22:28:07.394879       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="79.942µs"
I0405 22:28:07.406133       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="91.785µs"
I0405 22:28:07.409323       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="67.579µs"
I0405 22:28:10.421979       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="46.86µs"
I0405 22:28:24.827646       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="46.849µs"
I0405 22:28:40.826483       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="146.9µs"
I0405 22:28:54.829619       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="80.103µs"
I0405 22:29:11.825697       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="68.971µs"
I0405 22:29:25.823371       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="62.008µs"
I0405 22:29:54.837956       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="244.055µs"
I0405 22:30:06.828568       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="146.289µs"
I0405 22:30:36.657596       1 event.go:307] "Event occurred" object="default/second-image-6bbb48bfb7" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: second-image-6bbb48bfb7-4dzhv"
I0405 22:30:36.669574       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="36.520081ms"
I0405 22:30:36.680449       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="10.816818ms"
I0405 22:30:36.680609       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="92.085µs"
I0405 22:30:36.706106       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="83.92µs"
I0405 22:30:36.907761       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="36.57µs"
I0405 22:30:37.054696       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="145.367µs"
I0405 22:30:37.069519       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="68.04µs"
I0405 22:30:37.080846       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="69.342µs"
I0405 22:30:37.084083       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="70.385µs"
I0405 22:30:40.089562       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="40.387µs"
I0405 22:30:53.826345       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="56.898µs"
I0405 22:31:01.546861       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/second-image-6bbb48bfb7" duration="9.147µs"
I0405 22:31:47.169667       1 event.go:307] "Event occurred" object="default/hello-kube" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set hello-kube-59d66544fc to 1"
I0405 22:31:47.192397       1 event.go:307] "Event occurred" object="default/hello-kube-59d66544fc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: hello-kube-59d66544fc-76gv5"
I0405 22:31:47.200588       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/hello-kube-59d66544fc" duration="31.514197ms"
I0405 22:31:47.212749       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/hello-kube-59d66544fc" duration="12.049651ms"
I0405 22:31:47.212980       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/hello-kube-59d66544fc" duration="148.713µs"
I0405 22:31:47.228977       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/hello-kube-59d66544fc" duration="110.14µs"
I0405 22:31:49.878216       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/hello-kube-59d66544fc" duration="80.563µs"
I0405 22:32:00.825732       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/hello-kube-59d66544fc" duration="74.983µs"
I0405 22:32:13.824711       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/hello-kube-59d66544fc" duration="90.282µs"
I0405 22:32:27.827294       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/hello-kube-59d66544fc" duration="63.932µs"
I0405 22:32:41.824870       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/hello-kube-59d66544fc" duration="51.298µs"
I0405 22:32:53.825992       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/hello-kube-59d66544fc" duration="59.584µs"
I0405 22:33:32.825023       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/hello-kube-59d66544fc" duration="84.691µs"
I0405 22:33:46.830597       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/hello-kube-59d66544fc" duration="96.964µs"
I0405 22:35:04.828371       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/hello-kube-59d66544fc" duration="90.493µs"
I0405 22:35:16.826842       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/hello-kube-59d66544fc" duration="92.917µs"


==> kube-controller-manager [cb5d01d4b161] <==
I0405 21:21:00.470378       1 clusterroleaggregation_controller.go:189] "Starting ClusterRoleAggregator controller"
I0405 21:21:00.470387       1 shared_informer.go:311] Waiting for caches to sync for ClusterRoleAggregator
I0405 21:21:00.476907       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0405 21:21:00.503959       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0405 21:21:00.506413       1 shared_informer.go:318] Caches are synced for service account
I0405 21:21:00.509304       1 shared_informer.go:318] Caches are synced for crt configmap
I0405 21:21:00.512209       1 shared_informer.go:318] Caches are synced for job
I0405 21:21:00.517676       1 shared_informer.go:318] Caches are synced for namespace
I0405 21:21:00.518195       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0405 21:21:00.519594       1 shared_informer.go:318] Caches are synced for attach detach
I0405 21:21:00.523225       1 shared_informer.go:318] Caches are synced for TTL after finished
I0405 21:21:00.524449       1 shared_informer.go:318] Caches are synced for deployment
I0405 21:21:00.527770       1 shared_informer.go:318] Caches are synced for HPA
I0405 21:21:00.528401       1 shared_informer.go:318] Caches are synced for PVC protection
I0405 21:21:00.534113       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0405 21:21:00.535472       1 shared_informer.go:318] Caches are synced for node
I0405 21:21:00.535483       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0405 21:21:00.535602       1 range_allocator.go:174] "Sending events to api server"
I0405 21:21:00.535497       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0405 21:21:00.535642       1 range_allocator.go:178] "Starting range CIDR allocator"
I0405 21:21:00.535761       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0405 21:21:00.535775       1 shared_informer.go:318] Caches are synced for cidrallocator
I0405 21:21:00.536547       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0405 21:21:00.542029       1 shared_informer.go:318] Caches are synced for persistent volume
I0405 21:21:00.544389       1 shared_informer.go:318] Caches are synced for daemon sets
I0405 21:21:00.545720       1 shared_informer.go:318] Caches are synced for PV protection
I0405 21:21:00.547231       1 shared_informer.go:318] Caches are synced for expand
I0405 21:21:00.549161       1 shared_informer.go:318] Caches are synced for ephemeral
I0405 21:21:00.551444       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0405 21:21:00.552749       1 shared_informer.go:318] Caches are synced for cronjob
I0405 21:21:00.557253       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0405 21:21:00.560697       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0405 21:21:00.564641       1 shared_informer.go:318] Caches are synced for ReplicationController
I0405 21:21:00.566968       1 shared_informer.go:318] Caches are synced for taint
I0405 21:21:00.567031       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0405 21:21:00.567069       1 taint_manager.go:211] "Sending events to api server"
I0405 21:21:00.567143       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0405 21:21:00.567068       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0405 21:21:00.567226       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0405 21:21:00.567362       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/kubernetes-bootcamp-65df967b7f" duration="79.411µs"
I0405 21:21:00.567371       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0405 21:21:00.567394       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/kubernetes-bootcamp-f95c5b745" duration="94.961µs"
I0405 21:21:00.567410       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/kubernetes-bootcamp-7497bc6797" duration="137.181µs"
I0405 21:21:00.567439       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0405 21:21:00.570524       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0405 21:21:00.593609       1 shared_informer.go:318] Caches are synced for endpoint
I0405 21:21:00.596328       1 shared_informer.go:318] Caches are synced for TTL
I0405 21:21:00.600823       1 shared_informer.go:318] Caches are synced for GC
I0405 21:21:00.602062       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0405 21:21:00.619425       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="52.109289ms"
I0405 21:21:00.619581       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="67.368µs"
I0405 21:21:00.649504       1 shared_informer.go:318] Caches are synced for stateful set
I0405 21:21:00.698574       1 shared_informer.go:318] Caches are synced for disruption
I0405 21:21:00.740444       1 shared_informer.go:318] Caches are synced for resource quota
I0405 21:21:00.777460       1 shared_informer.go:318] Caches are synced for resource quota
I0405 21:21:01.118271       1 shared_informer.go:318] Caches are synced for garbage collector
I0405 21:21:01.138379       1 shared_informer.go:318] Caches are synced for garbage collector
I0405 21:21:01.138430       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0405 21:21:29.252727       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="12.996642ms"
I0405 21:21:29.252879       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="75.093µs"


==> kube-proxy [022ee63468b0] <==
I0405 22:08:50.100225       1 server_others.go:69] "Using iptables proxy"
I0405 22:08:50.119079       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0405 22:08:50.294981       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0405 22:08:50.299367       1 server_others.go:152] "Using iptables Proxier"
I0405 22:08:50.299449       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0405 22:08:50.299465       1 server_others.go:438] "Defaulting to no-op detect-local"
I0405 22:08:50.299543       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0405 22:08:50.299881       1 server.go:846] "Version info" version="v1.28.3"
I0405 22:08:50.299911       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0405 22:08:50.301236       1 config.go:188] "Starting service config controller"
I0405 22:08:50.301269       1 config.go:97] "Starting endpoint slice config controller"
I0405 22:08:50.301282       1 shared_informer.go:311] Waiting for caches to sync for service config
I0405 22:08:50.301283       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0405 22:08:50.307186       1 config.go:315] "Starting node config controller"
I0405 22:08:50.307779       1 shared_informer.go:311] Waiting for caches to sync for node config
I0405 22:08:50.402966       1 shared_informer.go:318] Caches are synced for service config
I0405 22:08:50.402973       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0405 22:08:50.409133       1 shared_informer.go:318] Caches are synced for node config


==> kube-proxy [e00d3a6b3948] <==
I0405 21:20:50.498107       1 server_others.go:69] "Using iptables proxy"
I0405 21:20:50.545370       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0405 21:20:50.695440       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0405 21:20:50.698661       1 server_others.go:152] "Using iptables Proxier"
I0405 21:20:50.698733       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0405 21:20:50.698743       1 server_others.go:438] "Defaulting to no-op detect-local"
I0405 21:20:50.700611       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0405 21:20:50.701189       1 server.go:846] "Version info" version="v1.28.3"
I0405 21:20:50.701212       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0405 21:20:50.707789       1 config.go:315] "Starting node config controller"
I0405 21:20:50.707822       1 shared_informer.go:311] Waiting for caches to sync for node config
I0405 21:20:50.708081       1 config.go:97] "Starting endpoint slice config controller"
I0405 21:20:50.708122       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0405 21:20:50.708664       1 config.go:188] "Starting service config controller"
I0405 21:20:50.708725       1 shared_informer.go:311] Waiting for caches to sync for service config
I0405 21:20:50.808416       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0405 21:20:50.808511       1 shared_informer.go:318] Caches are synced for node config
I0405 21:20:50.809598       1 shared_informer.go:318] Caches are synced for service config


==> kube-scheduler [1b9f043df7f9] <==
I0405 22:08:45.135672       1 serving.go:348] Generated self-signed cert in-memory
W0405 22:08:47.694023       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0405 22:08:47.694063       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0405 22:08:47.694079       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0405 22:08:47.694090       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0405 22:08:47.713132       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0405 22:08:47.713181       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0405 22:08:47.720683       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0405 22:08:47.720832       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0405 22:08:47.720865       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0405 22:08:47.720893       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0405 22:08:47.823046       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [d3b3071b5179] <==
I0405 21:20:45.421324       1 serving.go:348] Generated self-signed cert in-memory
I0405 21:20:47.925240       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0405 21:20:47.925284       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0405 21:20:47.996732       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0405 21:20:47.996767       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0405 21:20:47.996796       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0405 21:20:47.996830       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0405 21:20:47.998583       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0405 21:20:48.002530       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0405 21:20:48.002621       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0405 21:20:48.002632       1 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0405 21:20:48.099100       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0405 21:20:48.199483       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0405 21:20:48.203602       1 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
I0405 21:54:00.998889       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0405 21:54:00.999221       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
E0405 21:54:00.999488       1 run.go:74] "command failed" err="finished without leader elect"
I0405 21:54:00.999554       1 requestheader_controller.go:183] Shutting down RequestHeaderAuthRequestController


==> kubelet <==
Apr 05 22:33:58 minikube kubelet[1644]: E0405 22:33:58.517032    1644 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log\": failed to reopen container log \"38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:33:58 minikube kubelet[1644]: E0405 22:33:58.813430    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-image\" with ImagePullBackOff: \"Back-off pulling image \\\"second-image\\\"\"" pod="default/hello-kube-59d66544fc-76gv5" podUID="a22ed6d0-9fc7-46a9-92f2-b79187b37f01"
Apr 05 22:34:08 minikube kubelet[1644]: E0405 22:34:08.547930    1644 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:34:08 minikube kubelet[1644]: E0405 22:34:08.548035    1644 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log\": failed to reopen container log \"38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:34:11 minikube kubelet[1644]: E0405 22:34:11.812933    1644 kuberuntime_manager.go:1256] container &Container{Name:hello-kube,Image:hello-kube,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2z5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-kube_default(dcfe7190-90d1-49ea-903d-83e3193568f7): ErrImageNeverPull: Container image "hello-kube" is not present with pull policy of Never
Apr 05 22:34:11 minikube kubelet[1644]: E0405 22:34:11.812988    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-image\" with ImagePullBackOff: \"Back-off pulling image \\\"second-image\\\"\"" pod="default/hello-kube-59d66544fc-76gv5" podUID="a22ed6d0-9fc7-46a9-92f2-b79187b37f01"
Apr 05 22:34:11 minikube kubelet[1644]: E0405 22:34:11.813022    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-kube\" with ErrImageNeverPull: \"Container image \\\"hello-kube\\\" is not present with pull policy of Never\"" pod="default/hello-kube" podUID="dcfe7190-90d1-49ea-903d-83e3193568f7"
Apr 05 22:34:18 minikube kubelet[1644]: E0405 22:34:18.577965    1644 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:34:18 minikube kubelet[1644]: E0405 22:34:18.578077    1644 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log\": failed to reopen container log \"38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:34:24 minikube kubelet[1644]: E0405 22:34:24.813031    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-image\" with ImagePullBackOff: \"Back-off pulling image \\\"second-image\\\"\"" pod="default/hello-kube-59d66544fc-76gv5" podUID="a22ed6d0-9fc7-46a9-92f2-b79187b37f01"
Apr 05 22:34:24 minikube kubelet[1644]: E0405 22:34:24.813035    1644 kuberuntime_manager.go:1256] container &Container{Name:hello-kube,Image:hello-kube,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2z5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-kube_default(dcfe7190-90d1-49ea-903d-83e3193568f7): ErrImageNeverPull: Container image "hello-kube" is not present with pull policy of Never
Apr 05 22:34:24 minikube kubelet[1644]: E0405 22:34:24.813111    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-kube\" with ErrImageNeverPull: \"Container image \\\"hello-kube\\\" is not present with pull policy of Never\"" pod="default/hello-kube" podUID="dcfe7190-90d1-49ea-903d-83e3193568f7"
Apr 05 22:34:28 minikube kubelet[1644]: E0405 22:34:28.607745    1644 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:34:28 minikube kubelet[1644]: E0405 22:34:28.607853    1644 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log\": failed to reopen container log \"38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:34:36 minikube kubelet[1644]: E0405 22:34:36.813421    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-image\" with ImagePullBackOff: \"Back-off pulling image \\\"second-image\\\"\"" pod="default/hello-kube-59d66544fc-76gv5" podUID="a22ed6d0-9fc7-46a9-92f2-b79187b37f01"
Apr 05 22:34:38 minikube kubelet[1644]: E0405 22:34:38.634985    1644 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:34:38 minikube kubelet[1644]: E0405 22:34:38.635078    1644 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log\": failed to reopen container log \"38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:34:38 minikube kubelet[1644]: E0405 22:34:38.813601    1644 kuberuntime_manager.go:1256] container &Container{Name:hello-kube,Image:hello-kube,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2z5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-kube_default(dcfe7190-90d1-49ea-903d-83e3193568f7): ErrImageNeverPull: Container image "hello-kube" is not present with pull policy of Never
Apr 05 22:34:38 minikube kubelet[1644]: E0405 22:34:38.813701    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-kube\" with ErrImageNeverPull: \"Container image \\\"hello-kube\\\" is not present with pull policy of Never\"" pod="default/hello-kube" podUID="dcfe7190-90d1-49ea-903d-83e3193568f7"
Apr 05 22:34:48 minikube kubelet[1644]: E0405 22:34:48.664549    1644 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:34:48 minikube kubelet[1644]: E0405 22:34:48.664685    1644 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log\": failed to reopen container log \"38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:34:51 minikube kubelet[1644]: E0405 22:34:51.043468    1644 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for second-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="second-image:latest"
Apr 05 22:34:51 minikube kubelet[1644]: E0405 22:34:51.043549    1644 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: pull access denied for second-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="second-image:latest"
Apr 05 22:34:51 minikube kubelet[1644]: E0405 22:34:51.043655    1644 kuberuntime_manager.go:1256] container &Container{Name:second-image,Image:second-image,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8b9rr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-kube-59d66544fc-76gv5_default(a22ed6d0-9fc7-46a9-92f2-b79187b37f01): ErrImagePull: Error response from daemon: pull access denied for second-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Apr 05 22:34:51 minikube kubelet[1644]: E0405 22:34:51.043696    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-image\" with ErrImagePull: \"Error response from daemon: pull access denied for second-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/hello-kube-59d66544fc-76gv5" podUID="a22ed6d0-9fc7-46a9-92f2-b79187b37f01"
Apr 05 22:34:51 minikube kubelet[1644]: E0405 22:34:51.813163    1644 kuberuntime_manager.go:1256] container &Container{Name:hello-kube,Image:hello-kube,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2z5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-kube_default(dcfe7190-90d1-49ea-903d-83e3193568f7): ErrImageNeverPull: Container image "hello-kube" is not present with pull policy of Never
Apr 05 22:34:51 minikube kubelet[1644]: E0405 22:34:51.813240    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-kube\" with ErrImageNeverPull: \"Container image \\\"hello-kube\\\" is not present with pull policy of Never\"" pod="default/hello-kube" podUID="dcfe7190-90d1-49ea-903d-83e3193568f7"
Apr 05 22:34:58 minikube kubelet[1644]: E0405 22:34:58.693295    1644 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:34:58 minikube kubelet[1644]: E0405 22:34:58.693449    1644 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log\": failed to reopen container log \"38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:35:02 minikube kubelet[1644]: E0405 22:35:02.813616    1644 kuberuntime_manager.go:1256] container &Container{Name:hello-kube,Image:hello-kube,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2z5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-kube_default(dcfe7190-90d1-49ea-903d-83e3193568f7): ErrImageNeverPull: Container image "hello-kube" is not present with pull policy of Never
Apr 05 22:35:02 minikube kubelet[1644]: E0405 22:35:02.813707    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-kube\" with ErrImageNeverPull: \"Container image \\\"hello-kube\\\" is not present with pull policy of Never\"" pod="default/hello-kube" podUID="dcfe7190-90d1-49ea-903d-83e3193568f7"
Apr 05 22:35:04 minikube kubelet[1644]: E0405 22:35:04.813544    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-image\" with ImagePullBackOff: \"Back-off pulling image \\\"second-image\\\"\"" pod="default/hello-kube-59d66544fc-76gv5" podUID="a22ed6d0-9fc7-46a9-92f2-b79187b37f01"
Apr 05 22:35:08 minikube kubelet[1644]: E0405 22:35:08.725546    1644 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:35:08 minikube kubelet[1644]: E0405 22:35:08.725648    1644 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log\": failed to reopen container log \"38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:35:13 minikube kubelet[1644]: E0405 22:35:13.813911    1644 kuberuntime_manager.go:1256] container &Container{Name:hello-kube,Image:hello-kube,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2z5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-kube_default(dcfe7190-90d1-49ea-903d-83e3193568f7): ErrImageNeverPull: Container image "hello-kube" is not present with pull policy of Never
Apr 05 22:35:13 minikube kubelet[1644]: E0405 22:35:13.814006    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-kube\" with ErrImageNeverPull: \"Container image \\\"hello-kube\\\" is not present with pull policy of Never\"" pod="default/hello-kube" podUID="dcfe7190-90d1-49ea-903d-83e3193568f7"
Apr 05 22:35:16 minikube kubelet[1644]: E0405 22:35:16.813945    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-image\" with ImagePullBackOff: \"Back-off pulling image \\\"second-image\\\"\"" pod="default/hello-kube-59d66544fc-76gv5" podUID="a22ed6d0-9fc7-46a9-92f2-b79187b37f01"
Apr 05 22:35:18 minikube kubelet[1644]: E0405 22:35:18.756487    1644 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:35:18 minikube kubelet[1644]: E0405 22:35:18.756603    1644 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log\": failed to reopen container log \"38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:35:25 minikube kubelet[1644]: E0405 22:35:25.812977    1644 kuberuntime_manager.go:1256] container &Container{Name:hello-kube,Image:hello-kube,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2z5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-kube_default(dcfe7190-90d1-49ea-903d-83e3193568f7): ErrImageNeverPull: Container image "hello-kube" is not present with pull policy of Never
Apr 05 22:35:25 minikube kubelet[1644]: E0405 22:35:25.813031    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-kube\" with ErrImageNeverPull: \"Container image \\\"hello-kube\\\" is not present with pull policy of Never\"" pod="default/hello-kube" podUID="dcfe7190-90d1-49ea-903d-83e3193568f7"
Apr 05 22:35:28 minikube kubelet[1644]: E0405 22:35:28.900600    1644 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:35:28 minikube kubelet[1644]: E0405 22:35:28.900736    1644 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log\": failed to reopen container log \"38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:35:31 minikube kubelet[1644]: E0405 22:35:31.813736    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-image\" with ImagePullBackOff: \"Back-off pulling image \\\"second-image\\\"\"" pod="default/hello-kube-59d66544fc-76gv5" podUID="a22ed6d0-9fc7-46a9-92f2-b79187b37f01"
Apr 05 22:35:37 minikube kubelet[1644]: E0405 22:35:37.813037    1644 kuberuntime_manager.go:1256] container &Container{Name:hello-kube,Image:hello-kube,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2z5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-kube_default(dcfe7190-90d1-49ea-903d-83e3193568f7): ErrImageNeverPull: Container image "hello-kube" is not present with pull policy of Never
Apr 05 22:35:37 minikube kubelet[1644]: E0405 22:35:37.813154    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-kube\" with ErrImageNeverPull: \"Container image \\\"hello-kube\\\" is not present with pull policy of Never\"" pod="default/hello-kube" podUID="dcfe7190-90d1-49ea-903d-83e3193568f7"
Apr 05 22:35:39 minikube kubelet[1644]: E0405 22:35:39.000810    1644 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:35:39 minikube kubelet[1644]: E0405 22:35:39.000945    1644 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log\": failed to reopen container log \"38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:35:44 minikube kubelet[1644]: E0405 22:35:44.813312    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-image\" with ImagePullBackOff: \"Back-off pulling image \\\"second-image\\\"\"" pod="default/hello-kube-59d66544fc-76gv5" podUID="a22ed6d0-9fc7-46a9-92f2-b79187b37f01"
Apr 05 22:35:49 minikube kubelet[1644]: E0405 22:35:49.046881    1644 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:35:49 minikube kubelet[1644]: E0405 22:35:49.047035    1644 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log\": failed to reopen container log \"38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:35:49 minikube kubelet[1644]: E0405 22:35:49.813950    1644 kuberuntime_manager.go:1256] container &Container{Name:hello-kube,Image:hello-kube,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2z5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-kube_default(dcfe7190-90d1-49ea-903d-83e3193568f7): ErrImageNeverPull: Container image "hello-kube" is not present with pull policy of Never
Apr 05 22:35:49 minikube kubelet[1644]: E0405 22:35:49.814043    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-kube\" with ErrImageNeverPull: \"Container image \\\"hello-kube\\\" is not present with pull policy of Never\"" pod="default/hello-kube" podUID="dcfe7190-90d1-49ea-903d-83e3193568f7"
Apr 05 22:35:59 minikube kubelet[1644]: E0405 22:35:59.078389    1644 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:35:59 minikube kubelet[1644]: E0405 22:35:59.078513    1644 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log\": failed to reopen container log \"38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:35:59 minikube kubelet[1644]: E0405 22:35:59.813604    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"second-image\" with ImagePullBackOff: \"Back-off pulling image \\\"second-image\\\"\"" pod="default/hello-kube-59d66544fc-76gv5" podUID="a22ed6d0-9fc7-46a9-92f2-b79187b37f01"
Apr 05 22:36:02 minikube kubelet[1644]: E0405 22:36:02.813862    1644 kuberuntime_manager.go:1256] container &Container{Name:hello-kube,Image:hello-kube,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2z5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod hello-kube_default(dcfe7190-90d1-49ea-903d-83e3193568f7): ErrImageNeverPull: Container image "hello-kube" is not present with pull policy of Never
Apr 05 22:36:02 minikube kubelet[1644]: E0405 22:36:02.813968    1644 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"hello-kube\" with ErrImageNeverPull: \"Container image \\\"hello-kube\\\" is not present with pull policy of Never\"" pod="default/hello-kube" podUID="dcfe7190-90d1-49ea-903d-83e3193568f7"
Apr 05 22:36:09 minikube kubelet[1644]: E0405 22:36:09.109894    1644 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"
Apr 05 22:36:09 minikube kubelet[1644]: E0405 22:36:09.110035    1644 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log\": failed to reopen container log \"38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/kube-system_kube-apiserver-minikube_f9494db4e0276f36e16c209356ac7e81/kube-apiserver/4.log" containerID="38557dfd27ea11c47f7f1ed3de7d07fdb07c172efb7839db8014ca5b0479cdf4"


==> storage-provisioner [001f0e213c74] <==
I0405 22:08:49.796527       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0405 22:09:19.800320       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [e00eca37dec8] <==
I0405 22:09:34.956114       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0405 22:09:34.968154       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0405 22:09:34.968282       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0405 22:09:52.382048       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0405 22:09:52.382237       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_4d2fe68a-50ce-411a-824e-4027f08b896c!
I0405 22:09:52.382200       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"99390bc0-a96d-4448-871e-ad1cd743d94f", APIVersion:"v1", ResourceVersion:"24377", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_4d2fe68a-50ce-411a-824e-4027f08b896c became leader
I0405 22:09:52.483828       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_4d2fe68a-50ce-411a-824e-4027f08b896c!

